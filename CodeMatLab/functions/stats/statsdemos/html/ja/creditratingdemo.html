
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- This HTML was auto-generated from MATLAB code. To make changes, update the MATLAB code and republish this document.       --><title>決定木のバギングによる格付け</title><meta name="generator" content="MATLAB 7.14"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2011-11-14"><meta name="DC.source" content="creditratingdemo.m"><link rel="stylesheet" type="text/css" href="../../../../matlab/helptools/private/style.css"><link rel="stylesheet" type="text/css" href="../../../../matlab/helptools/private/style_ja_JP.css"></head><body><div class="header"><div class="left"><a href="matlab:edit creditratingdemo">エディターで creditratingdemo.m を開く</a></div><div class="right"><a href="matlab:echodemo creditratingdemo">コマンド ウィンドウで実行</a></div></div><div class="content"><h1>決定木のバギングによる格付け</h1><!--introduction--><p>信用リスク管理における基本的作業の 1 つは、借り手を格付けすることです。顧客をその認められた信用力に応じて格付けするために、&quot;等級&quot; が使用されます。等級が高いほど、リスクが低くなります。等級が近ければ、リスクのレベルも近いことになります。等級のカテゴリは、&quot;格付け&quot; と &quot;与信スコア&quot; の 2 つです。格付けは少数の別々のクラスで構成され、通常は &quot;AAA&quot; や &quot;BB-&quot; などのラベルが付けられます。与信スコアは &quot;640&quot; や &quot;720&quot; などの数値で表した等級です。信用度の等級は、規制の枠組み (Basel II など) における主要な要素の 1 つです (Basel II については、Basel Committee on Banking Supervision [3] を参照してください)。</p><p>信用格付けを行うには、借り手に関する情報を分析する必要があります。借り手が個人である場合、関心の対象となる情報はその個人の収入、未払い負債 (住宅ローンやクレジット カードなど)、世帯規模、住宅の状況などです。借り手が企業である場合、財務比率 (売上高を総資産で割った値など) や業種を考慮するかもしれません。ここでは借り手に関するこのような情報を &quot;特徴&quot; または &quot;予測子&quot; と呼びます<i></i><i></i>。格付け機関が異なれば、使用される予測子も異なります。また、顧客を格付けするための格付けクラスやスコア範囲も異なることがあります。規模の大きい借り手市場に比較的小額のローンを提供する場合 (クレジット カードなど)、信用スコアを使用するのが一般的であり、借り手を格付けするプロセスは通常は自動化されています。ローンが高額で、中小企業や大企業が利用できる場合、格付けを使用するのが一般的であり、自動化されたアルゴリズムと専門家の分析を組み合わせて格付けすることがあります。</p><p>企業の信用度の追跡を業務とする格付機関がいくつかあります。とは言え、大半の銀行は顧客を格付けするための独自の方法論を開発しています。ある顧客を社内で格付けすることが必要になるのは、その顧客がまだ格付機関によって格付けされたことがなく、たとえ第三者による格付けがあったとしても、社内で格付けすることにより顧客のリスク プロフィールの評価が補える場合です。</p><p>このデモでは、MATLAB&reg; が格付けプロセスの自動化された段階においてどのように役立つのかを示します。特に、Statistics Toolbox™ で既に提供されている統計学習ツールの 1 つである、&quot;バギングされた決定木&quot; として知られる分類決定木の一種を活用します。<i></i></p><p>履歴情報をデータセットの形で入手でき、各レコードには借り手の特徴と割り当てられた格付けが含まれているものとします。この格付けは、施行済みの手順とポリシーに従う委員会によって割り当てられた社内格付けであることもあります。または、格付機関による格付けのこともあります。そのような格付けは、社内格付けシステムを新規に始動するために使用されます。</p><p>このデモの出発点は既存の履歴データであり、そのデータを使用して格付けを自動化するバギングされた決定木を &quot;訓練&quot; します。<i></i>統計学習の語彙では、この訓練プロセスは &quot;教師あり学習&quot; のカテゴリに属します。<i></i>したがって、新規顧客の格付けには分類器が使用されます。実際問題としては、この自動化された格付け、つまり &quot;予測された&quot; 格付けは暫定的なものと見なされる可能性が高く、専門家で構成される信用調査委員会が審査して始めて確定します。<i></i>このデモで使用する種類の分類器は、このような格付けの見直しを促進する際にも使用することができます。予測された格付けの確実性の尺度、つまり &quot;分類スコア&quot; が提示されるからです。<i></i></p><p>実際問題としては、最初に行う必要があるのは分類器の訓練であり、次にその分類器を使用して新規顧客を格付けし、最後に分類器の質つまり &quot;精度のプロファイリングまたは評価&quot; を行う必要があります。このプロセスを &quot;検証&quot; または &quot;バックテスト&quot; といいます。<i></i><i></i><i></i><i></i>このデモでは、あらかじめ用意されていて入手可能なバックテスト ツールについても説明します。</p><!--/introduction--><h2>目次</h2><div><ul><li><a href="#1">既存の格付けデータの読み込み</a></li><li><a href="#4">ツリー バッガーの作成</a></li><li><a href="#9">新規データの分類</a></li><li><a href="#14">バックテスト: 分類プロセスのプロファイリング</a></li><li><a href="#23">おわりに</a></li><li><a href="#24">参考文献</a></li></ul></div><h2>既存の格付けデータの読み込み<a name="1"></a></h2><p>コンマ区切りテキスト ファイル <tt>CreditRating_Historical.dat</tt> から履歴データを読み込みます。ここでは、このファイル用のテキスト ファイルを操作することにしますが、Database Toolbox™ を使用できるユーザーであれば、この情報をデータベースから直接読み込むことができます。</p><p>データセットには、企業顧客リストに名を連ねる顧客の財務比率、業種、および格付けが含まれます。このデータは、実際のデータではなくシミュレーションされたものです。1 列目は顧客 ID です。それに続く 5 列は、財務比率です。これらの比率は Altman's z-score で使用される比率と同じです(Altman [1] を参照。関連する分析については Loeffler および Posch も参照)。</p><div><ul><li>運転資本/総資産 (WC_TA)</li></ul></div><div><ul><li>内部留保/総資産 (RE_TA)</li></ul></div><div><ul><li>税引前利払前利益/総資産 (EBIT_TA)</li></ul></div><div><ul><li>株式時価総額/全債務の簿価 (MVE_BVTD)</li></ul></div><div><ul><li>売上高/総資産 (S_TA)</li></ul></div><p>次に、業種ラベルを読み込みます。これは、1 ～ 12 の整数値で表されます。最終列には、顧客に割り当てられた格付けが表示されます。データを <tt>dataset</tt> 配列に読み込みます。</p><pre class="codeinput">creditDS = dataset(<span class="string">'file'</span>,<span class="string">'CreditRating_Historical.dat'</span>,<span class="string">'delimiter'</span>,<span class="string">','</span>);
</pre><p>特徴を行列 <tt>X</tt>、対応するクラス、格付けをベクトル <tt>Y</tt> にコピーします。この情報には <tt>dataset</tt> 配列から直接アクセスできるので、この手順は必須ではありません。ここでこの手順を実行するのは、これ以降で繰り返されるいくつかの関数呼び出しを単純化するためです。</p><p>行列 <tt>X</tt> に格納される特徴は、財務比率 5 つと業種ラベルです。<tt>Industry</tt> は実際にはカテゴリ変数の一種である &quot;ノミナル&quot; 変数<i></i>です。業種には順序は存在しないからです。これに対する応答変数、つまり格付けも、カテゴリ変数です。ただし、こちらは &quot;序数&quot; <i></i>変数です。定義では、格付けは信用度の &quot;ランキング&quot; <i></i>を示唆するからです。この変数をそのまま使用して、分類器を訓練することができます。ここでは、この変数を &quot;順序配列&quot; <i></i>にコピーすることにします。こうすることにより、格付けの順序通りに出力されて読みやすくなるからです。格付けの順序は、<tt>Y</tt> の定義の3 番目の引数として渡すセル配列によって確立されます。格付けは、数値にマッピングすることもできます。数値へのマッピングは、別のデータ解析方法 (回帰分析など) を試すときに便利です。実際問題として、さまざまな方法を試すことをお勧めします。</p><pre class="codeinput">X = [creditDS.WC_TA creditDS.RE_TA creditDS.EBIT_TA creditDS.MVE_BVTD<span class="keyword">...</span>
     creditDS.S_TA creditDS.Industry];
Y = ordinal(creditDS.Rating,[],{<span class="string">'AAA'</span> <span class="string">'AA'</span> <span class="string">'A'</span> <span class="string">'BBB'</span> <span class="string">'BB'</span> <span class="string">'B'</span> <span class="string">'CCC'</span>});
</pre><p>予測子 <tt>X</tt> と応答変数 <tt>Y</tt> を使用して、&quot;バギングされた決定木&quot; と呼ばれる特定の種類の分類集合を適合させます。<i></i>この文脈では、&quot;バギング&quot; は &quot;ブートストラップ集団&quot; を意味します。この方法論の本質は、データセットから副サンプル、つまり &quot;ブートストラップ レプリカ&quot; をいくつか生成することにあります。<i></i>これらの副サンプルはランダムに生成されます。これは、データセット内の顧客リストに基づく置換によるサンプリングです。レプリカごとに決定木が 1 つ成長します。各決定木は、独自に訓練された分類器であり、新規顧客の分類に単独で使用することができます。ただし、2 つのブートストラップ レプリカから成長した 2 本の木の予測は異なる可能性があります。この集合は、ブートストラップ レプリカすべてに対して成長した決定木すべての予測を &quot;集約&quot; します。<i></i>決定木の大多数が、ある新規顧客について 1 つのクラスを予測した場合、その予測は単独の決定木による予測より確実であると考えるのは、道理にかなっています。さらに、少数の決定木が別のクラスを予測した場合、その情報も有益です。実際、別のクラスを予測する決定木の割合は、新規データの分類時に分類集合により報告される &quot;分類スコア&quot; の土台となります。<i></i></p><h2>ツリー バッガーの作成<a name="4"></a></h2><p>分類集合を作成する最初の手順は、個別の決定木に適したリーフ サイズを見つけることです。ここでは、1、5、および 10 のサイズを試します (『Statistics Toolbox ユーザー ガイド』の <tt>TreeBagger</tt> リファレンス ページと「決定木のバギングによる回帰と分類」を参照してください)。最初は少数の 25 本の決定木から始めます。主要な目的が、さまざまなリーフ サイズでの分類誤差の初期傾向を比較することだからです。再現可能性と公平な比較を確保するため、乱数発生器を再初期化します。乱数発生器は分類器を作成するたびにデータを置換してサンプリングするために使用されます。</p><pre class="codeinput">leaf = [1 5 10];
nTrees = 25;
rng(9876,<span class="string">'twister'</span>);
savedRng = rng; <span class="comment">% save the current RNG settings</span>

color = <span class="string">'bgr'</span>;
<span class="keyword">for</span> ii = 1:length(leaf)
   <span class="comment">% Reinitialize the random number generator, so that the</span>
   <span class="comment">% random samples are the same for each leaf size</span>
   rng(savedRng);
   <span class="comment">% Create a bagged decision tree for each leaf size and plot out-of-bag</span>
   <span class="comment">% error 'oobError'</span>
   b = TreeBagger(nTrees,X,Y,<span class="string">'oobpred'</span>,<span class="string">'on'</span>,<span class="string">'cat'</span>,6,<span class="string">'minleaf'</span>,leaf(ii));
   plot(b.oobError,color(ii));
   hold <span class="string">on</span>;
<span class="keyword">end</span>
xlabel(<span class="string">'Number of grown trees'</span>);
ylabel(<span class="string">'Out-of-bag classification error'</span>);
legend({<span class="string">'1'</span>, <span class="string">'5'</span>, <span class="string">'10'</span>},<span class="string">'Location'</span>,<span class="string">'NorthEast'</span>);
title(<span class="string">'Classification Error for Different Leaf Sizes'</span>);
hold <span class="string">off</span>;
</pre><img vspace="5" hspace="5" src="../creditratingdemo_01.png" alt=""> <p>これら 3 種類のリーフ サイズについて、誤差を比較することができます。したがって、リーフ サイズ 10 について検討することにします。この方が無駄がなく、計算の効率性が高いからです。</p><p>データを &quot;訓練用&quot; と &quot;テスト用&quot; のサブセットに分割する必要はなかったことに注意してください。<i></i><i></i>この方法の根底にあるサンプリング手順で暗黙的ですが、これは内部的に行われます。ブートストラップを反復するたびに、ブートストラップ レプリカが訓練セットになります。除外された (&quot;out-of-bag&quot;) 顧客がいれば、その顧客は前に報告された out-of-bag 分類誤差を評価するためのテスト ポイントとして使用されます。</p><p>次に知りたいのは、使用している分類器の精度にとってあらゆる特徴が重要なのかどうかという点です。そのために、&quot;特徴の重要度&quot; 測定項目 (<tt>oobvarimp</tt>) を有効にし、最も重要な特徴を視覚的に見つけるために結果をプロットします。<i></i>また、決定木の本数を増やしてみて、分類誤差を保存します。この結果は後に示すようにさらに比較するために使用します。</p><pre class="codeinput">nTrees = 50;
leaf = 10;
rng(savedRng);
b = TreeBagger(nTrees,X,Y,<span class="string">'oobvarimp'</span>,<span class="string">'on'</span>,<span class="string">'cat'</span>,6,<span class="string">'minleaf'</span>,leaf);

bar(b.OOBPermutedVarDeltaError);
xlabel(<span class="string">'Feature number'</span>);
ylabel(<span class="string">'Out-of-bag feature importance'</span>);
title(<span class="string">'Feature importance results'</span>);

oobErrorFullX = b.oobError;
</pre><img vspace="5" hspace="5" src="../creditratingdemo_02.png" alt=""> <p>特徴 2、4、および 6 は、その他の特徴とは一線を画しています。特徴 4 は、株式時価総額/全債務の簿価 (<tt>MVE_BVTD</tt>) ですが、このデータセットにとって最も重要な予測子です。この比率は、Merton のモデル [5] など、構造モデルにおける信用度の予測子に密接に関係しています。ここで、債務不履行確率を判断するため、企業の株式の値を未払い負債と比較します。</p><p>業種情報の特徴 6 (<tt>Industry</tt>) も、このデータセットに関して企業の信用度を評価する点で、その他の変数より相対的に重要です。</p><p><tt>MVE_BVTD</tt> ほどには重要ではありませんが、特徴 2 (内部留保/総資産 (<tt>RE_TA</tt>)) もその他の特徴とは一線を画しています。内部留保と企業の存続年数には、相関関係があります (一般に、存続年数が長ければ長いほど、蓄積できる内部留保は増えます)。そして、企業の存続年数は信用度と相関関係があります (歴史の長い企業ほど、困難な時期を乗り越えられる可能性は高くなります)。</p><p>それでは、予測子 <tt>RE_TA</tt>、<tt>MVE_BVTD</tt>、および <tt>Industry</tt> のみを使用して新しい分類集合を適合させてみましょう。分類誤差と以前の分類器を比較します。この以前の分類器では特徴がすべて使用されます。</p><pre class="codeinput">X = [creditDS.RE_TA creditDS.MVE_BVTD creditDS.Industry];

rng(savedRng);
b = TreeBagger(nTrees,X,Y,<span class="string">'oobpred'</span>,<span class="string">'on'</span>,<span class="string">'cat'</span>,3,<span class="string">'minleaf'</span>,leaf);

oobErrorX246 = b.oobError;

plot(oobErrorFullX,<span class="string">'b'</span>);
hold <span class="string">on</span>;
plot(oobErrorX246,<span class="string">'r'</span>);
xlabel(<span class="string">'Number of grown trees'</span>);
ylabel(<span class="string">'Out-of-bag classification error'</span>);
legend({<span class="string">'All features'</span>, <span class="string">'Features 2, 4, 6'</span>},<span class="string">'Location'</span>,<span class="string">'NorthEast'</span>);
title(<span class="string">'Classification Error for Different Sets of Predictors'</span>);
hold <span class="string">off</span>;
</pre><img vspace="5" hspace="5" src="../creditratingdemo_03.png" alt=""> <p>重要度が相対的に低い特徴 (1、3、および 5) を除外しても分類精度は大きく低下しないので、予測には特徴を削減した分類集合を使用します。</p><p>この例では、6 個の特徴で構成されるセットで開始し、変数のうち 3 つをスクリーニングで除外するための基準として分類器の特徴重要度測定法と out-of-bag 分類誤差を使用しました。予測子の初期セットに変数が多数含まれる場合、特徴の選択に時間がかかってしまう可能性があります。ここで使用したツール (変数重要度と out-of-bag 誤差の &quot;視覚的&quot; 比較) 以外に、Statistics Toolbox のツール (<tt>sequentialfs</tt> など) もこの種の分析に役立つことがあります (Statistics Toolbox のデモ「高次元のデータを分類する特徴の選択」を参照してください)。ただし、結局のところ、特徴選択を成功させるには計量的分析手法に分析者の判断を加味することが必要になります。</p><p>たとえば、ここで使用した変数重要度測定法は、ある特徴の相対的な影響度を見積もるランク付けのしくみです。見積もるには、この特徴の値がランダムに置換された場合に分類器の予測精度がどの程度低下するのかを測定します。基本となる考え方は、問題の特徴が分類器の予測性能にあまり貢献しないのであれば、値を変更 (この場合は置換) して使用しても分類の結果は影響を受けないはずだ、というものです。一方、予測精度を低下させることなく関連情報をランダムに入れ替えることはできません。以上のことから、2 つの相関する特徴が重要である場合、いずれもこの分析で上位にランクインします。その場合、正確に分類するためにこれらの特徴の 1 つを維持することだけでも大変なのですが、そのことはランク付けの結果だけからはわからないでしょう。相関性を個別にチェックするか、専門家の判断を加味しなければならないでしょう。つまり、変数重要度や <tt>sequentialfs</tt> などの手段は特徴選択に大いに貢献する可能性はあるものの、このプロセスで最も重要なのは分析者の判断だ、ということです。</p><p>この時点で、新規顧客を分類するために今後のセッション (<tt>load classifier</tt>) で読み込むためにこの分類器を保存する (<tt>save classifier.mat b</tt> など) ことができます。効率性のため、訓練課程が終了したらコンパクト版の分類器を保存しておくことをお勧めします。</p><pre class="codeinput">b = b.compact;
</pre><h2>新規データの分類<a name="9"></a></h2><p>ここでは、以前に作成した分類集合を使用して新規顧客の信用を格付けします。既存顧客の格付けも定期的に見直す必要がある (特に、財務情報が実質的に更新されたとき) ので、データセットに見直し中の既存顧客のリストも含まれることがあります。新規データの読み込みから始めます。</p><pre class="codeinput">newDS = dataset(<span class="string">'file'</span>,<span class="string">'CreditRating_NewCompanies.dat'</span>,<span class="string">'delimiter'</span>,<span class="string">','</span>);
</pre><p>この新規データの格付けを予測するため、分類器に <tt>predict</tt> メソッドを呼び出します。このメソッドは、予測クラスと分類スコアという 2 つの引数を返します。このどちらの出力引数も必要なものです。分類スコアには予測された格付けの確実性に関する情報が含まれるからです。以前と同じように、変数 <tt>RE_TA</tt>、<tt>MVE_BVTD</tt>、および <tt>Industry</tt> を行列 <tt>X</tt> にコピーしてもよいのですが、呼び出すのは <tt>predict</tt> だけなので、この手順を省略して <tt>newDS</tt> を直接使用します。</p><pre class="codeinput">[predClass,classifScore] = b.predict([newDS.RE_TA newDS.MVE_BVTD<span class="keyword">...</span>
   newDS.Industry]);
</pre><p>この時点で、レポートを作成することができます。ここでは、説明のために画面に最初の 3 件の顧客に関する小さいレポートしか示しませんが、MATLAB の開発ツールを使用するとここでのワークフローを大幅に改善することができます。たとえば、クレジット アナリストは MATLAB Builder™ JA を使用してこの分類をリモートに実行し、Web ブラウザーでレポートを確認することができます。MATLAB がデスクトップにインストールされている必要はありません。</p><pre class="codeinput"><span class="keyword">for</span> i = 1:3
   fprintf(<span class="string">'Customer %d:\n'</span>,newDS.ID(i));
   fprintf(<span class="string">'   RE/TA    = %5.2f\n'</span>,newDS.RE_TA(i));
   fprintf(<span class="string">'   MVE/BVTD = %5.2f\n'</span>,newDS.MVE_BVTD(i));
   fprintf(<span class="string">'   Industry = %2d\n'</span>,newDS.Industry(i));
   fprintf(<span class="string">'   Predicted Rating : %s\n'</span>,predClass{i});
   fprintf(<span class="string">'   Classification score : \n'</span>);
   <span class="keyword">for</span> j = 1:length(b.ClassNames)
      <span class="keyword">if</span> (classifScore(i,j)&gt;0)
         fprintf(<span class="string">'      %s : %5.4f \n'</span>,b.ClassNames{j},classifScore(i,j));
      <span class="keyword">end</span>
   <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><pre class="codeoutput">Customer 60644:
   RE/TA    =  0.22
   MVE/BVTD =  2.40
   Industry =  6
   Predicted Rating : AA
   Classification score : 
      AA : 0.6418 
      A : 0.3299 
      BBB : 0.0282 
Customer 33083:
   RE/TA    =  0.24
   MVE/BVTD =  1.51
   Industry =  4
   Predicted Rating : BBB
   Classification score : 
      A : 0.0669 
      BBB : 0.9331 
Customer 63830:
   RE/TA    =  0.18
   MVE/BVTD =  1.69
   Industry =  7
   Predicted Rating : A
   Classification score : 
      AA : 0.0198 
      A : 0.6588 
      BBB : 0.3194 
      B : 0.0020 
</pre><p>予測された格付けおよび対応するスコアのレコードを保持することは、分類器の質を定期的に評価する際に役立ちます。ここではこの情報を <tt>dataset</tt> 配列である <tt>predDS</tt> に保存します。</p><pre class="codeinput">predDS = dataset({newDS.ID,<span class="string">'ID'</span>},{predClass,<span class="string">'PredRating'</span>},<span class="keyword">...</span>
   {classifScore,<span class="string">'sAAA'</span>,<span class="string">'sAA'</span>,<span class="string">'sA'</span>,<span class="string">'sBBB'</span>,<span class="string">'sBB'</span>,<span class="string">'sB'</span>,<span class="string">'sCCC'</span>});
</pre><p>この情報は、たとえば次のコマンドを使用して、コンマ区切りテキスト ファイル <tt>PredictedRatings.dat</tt> に保存することもできます。</p><pre>  export(predDS,'file','PredictedRatings.dat','delimiter',',')</pre><p>または、Database Toolbox を使用してデータベースに直接書き込むこともできます。</p><h2>バックテスト: 分類プロセスのプロファイリング<a name="14"></a></h2><p>格付けの質をプロファイリングつまり評価するプロセスを &quot;検証&quot; または &quot;バックテスト&quot; といいます。<i></i><i></i>この作業に関係する測定法とテストはいくつもあります (たとえば、バーゼル銀行監督委員会 [2] を参照してください)。このデモでは、次の 2 つの問題に焦点を当てます。</p><div><ul><li>予測された格付けは実際の格付けと比較してどの程度正確なのでしょうか。ここでいう &quot;予測された格付け&quot; とは、自動化された分類プロセスで得られる格付けを指します。&quot;実際の格付け&quot; とは、信用調査委員会が割り当てる格付けを指します。信用調査委員会は、予測された格付けとその分類スコア、およびその他の情報 (ニュースや経済状況など) を総合的に判断して最終的なランキングを決定します。</li></ul></div><div><ul><li>実際の格付けは顧客を信用度に従ってどの程度正確にランク付けすることができるのでしょうか。これが行われるのは、<i>ex-post</i> (事後) 分析 (1 年後など) においてです。これくらい時間が経過すれば、その期間中に債務不履行に陥った企業が明らかになるからです。</li></ul></div><p>ファイル <tt>CreditRating_ExPost.dat</tt> には、前の節で検討した同じ企業に関する &quot;追跡&quot; データが含まれます。このファイルには、委員会がそれらの企業に割り当てた実際の格付けおよび &quot;債務不履行フラグ&quot; が含まれます。債務不履行フラグとは、格付けから 1 年以内にその企業が債務不履行に陥った (1) かそうでない (0) かを示す指標です。</p><pre class="codeinput">exPostDS = dataset(<span class="string">'file'</span>,<span class="string">'CreditRating_ExPost.dat'</span>,<span class="string">'delimiter'</span>,<span class="string">','</span>);
</pre><p><tt>CreditRating_ExPost.dat</tt> から追跡データを読み取ることに加えて、これが新しい MATLAB セッションであれば、たとえば次のコマンドを使用して、<tt>predDS</tt> に保存された予測された格付け情報を読み込む必要があります。</p><pre>  predDS = dataset('file','PredictedRatings.dat','delimiter',',')</pre><p><b>予測された格付け対実際の格付けの比較。</b>自動化された分類器を訓練する根拠は、信用調査委員会の作業を促進するためです。予測された格付けが正確であればあるほど、委員会が予測された格付けの見直しに費やさねばならない時間は短縮されます。したがって、予測された格付けが実際に割り当てられた最終格付けにどれほど近いのかを定期的にチェックすること、および食い違いが大きければ、自動化された分類器を再訓練するよう勧めること (さらに、たとえば新機能の搭載) を委員会が希望するのは考えられることです。</p><p>予測された格付けと実際の格付けを比較するために使用できる最初のツールは、<i>confusion atrix</i> です。これは、Statistics Toolbox にあらかじめ用意されています。</p><pre class="codeinput">C = confusionmat(exPostDS.Rating,predDS.PredRating,<span class="keyword">...</span>
   <span class="string">'order'</span>,{<span class="string">'AAA'</span> <span class="string">'AA'</span> <span class="string">'A'</span> <span class="string">'BBB'</span> <span class="string">'BB'</span> <span class="string">'B'</span> <span class="string">'CCC'</span>})
</pre><pre class="codeoutput">
C =

   203    13     0     0     0     0     0
     8   109    21     0     0     0     0
     0    18   154    32     0     0     0
     0     1    26   253    43     0     0
     0     0     0    41   226    18     0
     0     0     0     0    47    43     3
     0     0     0     0     0    10    42

</pre><p><tt>C</tt> の行は実際の格付け、列は予測された格付けに対応します。この行列内の位置 <tt>(i,j)</tt> における値は、実際の格付け <tt>i</tt> を受けた顧客の数および格付けが <tt>j</tt> と予測された顧客の数を示します。たとえば、位置 <tt>(3,2)</tt> は信用調査委員会が &quot;A&quot; と格付けした顧客の数および自動化された分類器で &quot;AA&quot; と予測された顧客の数を示します。この行列を次の簡単な変換でパーセンテージ形式で表現することもできます。</p><pre class="codeinput">Cperc = diag(sum(C,2))\C
</pre><pre class="codeoutput">
Cperc =

    0.9398    0.0602         0         0         0         0         0
    0.0580    0.7899    0.1522         0         0         0         0
         0    0.0882    0.7549    0.1569         0         0         0
         0    0.0031    0.0805    0.7833    0.1331         0         0
         0         0         0    0.1439    0.7930    0.0632         0
         0         0         0         0    0.5054    0.4624    0.0323
         0         0         0         0         0    0.1923    0.8077

</pre><p>予測された格付けと実際の格付けがよく一致していれば、同じ行内のその他の値を左右する主対角要素の値が、理想的には 1 に近い値になります。この場合、&quot;B&quot; について影響の大きい不一致が実際に見られます。信用調査委員会が &quot;B&quot; と格付けした顧客の約半数が、自動化された分類器によって &quot;BB&quot; と予測されていたためです。一方、唯一の例外の &quot;BBB&quot; を除き、大半のケースで格付けの食い違いがせいぜい 1 段階でしかないのは良いことです。</p><p>混合行列を使用して、格付け機関による社内格付けと第三者の格付けを比較することもできます。これは実際によく行われます。</p><p>特定の格付けについて、予測された格付けと実際の格付けの一致度をさらに別途算出することができます。Statistics Toolbox の関数 <tt>perfcurve</tt> を使用して &quot;受信者動作特性 (ROC) 曲線&quot; を描き、&quot;曲線下面積 (AUC)&quot; をチェックすることができます。<i></i><i></i>関数 <tt>perfcurve</tt> は、実際の格付け (これがベンチマークです)、比較対象とする基準、および自動化されたプロセスによって算出された 'BBB' 分類スコアという引数を取ります。それでは、ROC を作成しこの例で格付け 'BBB' の AUC を計算しましょう。</p><pre class="codeinput">[xVal,yVal,~,auc] = perfcurve(exPostDS.Rating,predDS.sBBB,<span class="string">'BBB'</span>);
plot(xVal,yVal);
xlabel(<span class="string">'False positive rate'</span>);
ylabel(<span class="string">'True positive rate'</span>);
text(0.5,0.25,strcat(<span class="string">'AUC='</span>,num2str(auc)),<span class="string">'EdgeColor'</span>,<span class="string">'k'</span>);
title(<span class="string">'ROC curve BBB, predicted vs. actual rating'</span>);
</pre><img vspace="5" hspace="5" src="../creditratingdemo_04.png" alt=""> <p>ROC の作成方法を説明します。自動化された分類器が顧客ごとに各格付け、特に 'BBB' の分類スコアを返すことを思い出してください。このスコアは、この特定の顧客が 'BBB' とランク付けされる可能性の高さを表すと解釈することができます。ROC 曲線を作成するには、&quot;分類しきい値&quot; に変化をもたせる必要があります。<i></i>分類しきい値とは、ある顧客を 'BBB' と分類するための最小スコアのことです。つまり、しきい値が <tt>t</tt> の場合、'BBB' スコアが <tt>t</tt> 以上の場合のみ、その顧客を 'BBB' に分類します。たとえば、<i>XYZ</i> 社の 'BBB' スコアが 0.87 だとします。<i>XYZ</i> 社の実際の格付け (<tt>exPostDS.Rating</tt> で示された情報) が 'BBB' である場合、<i>XYZ</i> 社は最高 0.87 までの任意のしきい値について正しく 'BBB' と分類されることになります。これは &quot;真陽性&quot; であり、分類器のいわゆる &quot;感度&quot; が向上します。<i></i><i></i>しきい値が 0.87 を超える場合、同社は 'BBB' に格付けされず、&quot;偽陰性&quot; となります。<i></i>説明を補足するため、<i>XYZ</i> 社の実際の格付けが 'BB' であるとしましょう。すると、0.87 を超えるしきい値に対しては 'BBB' として正しく却下され、&quot;真陰性&quot; となり、<i></i>こうして分類器のいわゆる &quot;特異性&quot; が向上します。<i></i>しかし、0.87 までのしきい値に対しては &quot;偽陽性&quot; になります (実際には 'BB' でも、'BBB' に分類されます)。<i></i>ROC 曲線を作成するには、しきい値が 0 ～ 1 の間を動くものとして、真陽性 (感度) と偽陰性 (1 特異度) の比の値をプロットしていきます。</p><p>AUC はその名前が示すとおり、ROC 曲線の下の面積です。AUC が 1 に近ければ近いほど、分類器の精度は高くなります (分類器が完全であれば、AUC は 1)。この例では、AUC は十分に高いようですが、どのレベルの AUC だと自動化された分類器を改良するよう勧告するのかを決定するのは信用調査委員会です。</p><p><b>実際の格付け対翌年の債務不履行件数。</b>格付けにおいて暗黙的な顧客のランキングを評価するために使用される一般的な指標は、&quot;累積精度輪郭 (CAP)&quot; およびそれに関連する &quot;精度率&quot; です。<i></i><i></i>基本的な考え方は、割り当てられた格付けと翌年に観察された債務不履行件数との関係を測定する、というものです。格付けのクラスが良いほど、債務不履行の件数は減少するものと思われます。どの格付けでも貸倒発生率が同じであれば、その格付けシステムは単純で役立たずの分類システムであり、顧客は信用度とは無関係にランダムに格付けされていることになります。</p><p>関数 <tt>perfcurve</tt> を CAP の作成にも使用できることはすぐにわかるはずです。比較対象とする基準は、以前とは異なり格付けではなく、<tt>CreditRating_ExPost.dat</tt> ファイルから読み込んだ債務不履行フラグです。使用するスコアは &quot;ダミー スコア&quot; で、これは格付けリストで暗黙的な信用度のランキングを示します。ダミー スコアが満たすべき条件は、格が上がるほどダミー スコアは下がる (&quot;債務不履行フラグが 1 になる可能性は低い&quot; という意味)、および 2 つの顧客の格が同じであればダミー スコアも同点になる、ということだけです。債務不履行確率は、もちろんスコアとして渡すこともできますが、ここでは債務不履行確率は不明です。実際のところ、&quot;債務不履行確率の推定値がなくても、CAP を作成できます&quot;。債務不履行確率を検証しているわけではないからです。<i></i>この指標で評価しているのは、格付けにより顧客をその信用度に従って &quot;ランク付け&quot; するときの精度です。<i></i></p><p>通常は、検討対象の格付けシステムの CAP を &quot;完全な格付けシステム&quot; の CAP と共にプロットします。後者は仮説上の格付けシステムであり、格が最低のクラスにはすべての不履行者が分類され、その他の顧客は最低クラスには入れられません。この完全な曲線の下の面積は、格付けシステムが達成し得る AUC の最大値になります。慣例により、&quot;単純なシステム&quot; の CAP の下の面積を減算するためのさまざまな CAP について、AUC が調整されます。&quot;単純なシステム&quot; の CAP とは、顧客をランダムに格付けするシステムの CAP のことです。<i></i>単純なシステムの CAP は、原点から座標 (1,1) に伸びる直線であり、AUC は 0.5 です。したがって、格付けシステムの &quot;精度率&quot; の定義は、調整済み ACU (検討対象システムの AUC から単純なシステムの AUC を引いたもの) と最高精度 (完全なシステムの ACU から単純なシステムの AUC を引いたもの) の比率です。<i></i></p><pre class="codeinput">ratingsList = {<span class="string">'AAA'</span> <span class="string">'AA'</span> <span class="string">'A'</span> <span class="string">'BBB'</span> <span class="string">'BB'</span> <span class="string">'B'</span> <span class="string">'CCC'</span>};
Nratings = length(ratingsList);
dummyDelta = 1/(Nratings+1);
dummyRank = linspace(dummyDelta,1-dummyDelta,Nratings)';

D = exPostDS.Def_tplus1;
fracTotDef = sum(D)/length(D);
maxAcc = 0.5 - 0.5 * fracTotDef;

R = double(ordinal(exPostDS.Rating,[],ratingsList));
S = dummyRank(R);
[xVal,yVal,~,auc] = perfcurve(D,S,1);

accRatio = (auc-0.5)/maxAcc;
fprintf(<span class="string">'Accuracy ratio for actual ratings: %5.3f\n'</span>,accRatio);

xPerfect(1) = 0; xPerfect(2) = fracTotDef; xPerfect(3) = 1;
yPerfect(1) = 0; yPerfect(2) = 1; yPerfect(3) = 1;
xNaive(1) = 0; xNaive(2) = 1;
yNaive(1) = 0; yNaive(2) = 1;

plot(xPerfect,yPerfect,<span class="string">'--k'</span>,xVal,yVal,<span class="string">'b'</span>,xNaive,yNaive,<span class="string">'-.k'</span>);
xlabel(<span class="string">'Fraction of all companies'</span>);
ylabel(<span class="string">'Fraction of defaulted companies'</span>);
title(<span class="string">'Cumulative Accuracy Profile'</span>);
legend({<span class="string">'Perfect'</span>,<span class="string">'Actual'</span>,<span class="string">'Naive'</span>},<span class="string">'Location'</span>,<span class="string">'SouthEast'</span>);
text(xVal(2)+0.01,yVal(2)-0.01,<span class="string">'CCC'</span>)
text(xVal(3)+0.01,yVal(3)-0.02,<span class="string">'B'</span>)
text(xVal(4)+0.01,yVal(4)-0.03,<span class="string">'BB'</span>)
</pre><pre class="codeoutput">Accuracy ratio for actual ratings: 0.850
</pre><img vspace="5" hspace="5" src="../creditratingdemo_05.png" alt=""> <p>CAP の情報を読み取るポイントは、&quot;ねじれ&quot; にあります。このねじれというレッテルは、格付け 'CCC'、'B'、および 'BB' のプロットで貼られます。たとえば、2 番目のねじれは 2 番目に低い格付け 'B' に関連付けられており、(0.097, 0.714) にあります。これは、顧客の 9.7% が 'B'、つまり &quot;比較的低い&quot; とランク付けされ、観察された債務不履行件数の 71.4% を占めることを意味します。<i></i></p><p>一般に、精度率は絶対的な測定値ではなく相対値として処理すべきです。たとえば、予測された格付けの CAP をサンプル プロットに加算し、精度率を計算して、実際の格付けの精度率と比較することができます。</p><pre class="codeinput">Rpred = double(ordinal(predDS.PredRating,[],ratingsList));
Spred = dummyRank(Rpred);
[xValPred,yValPred,~,aucPred] = perfcurve(D,Spred,1);

accRatioPred = (aucPred-0.5)/maxAcc;
fprintf(<span class="string">'Accuracy ratio for predicted ratings: %5.3f\n'</span>,accRatioPred);

plot(xPerfect,yPerfect,<span class="string">'--k'</span>,xVal,yVal,<span class="string">'b'</span>,xNaive,yNaive,<span class="string">'-.k'</span>,<span class="keyword">...</span>
   xValPred,yValPred,<span class="string">':r'</span>);
xlabel(<span class="string">'Fraction of all companies'</span>);
ylabel(<span class="string">'Fraction of defaulted companies'</span>);
title(<span class="string">'Cumulative Accuracy Profile'</span>);
legend({<span class="string">'Perfect'</span>,<span class="string">'Actual'</span>,<span class="string">'Naive'</span>,<span class="string">'Predicted'</span>},<span class="string">'Location'</span>,<span class="string">'SouthEast'</span>);
</pre><pre class="codeoutput">Accuracy ratio for predicted ratings: 0.817
</pre><img vspace="5" hspace="5" src="../creditratingdemo_06.png" alt=""> <p>予測された格付けの精度率は下がり、大半のケースでその CAP は実際の格付けの CAP を下回ります。これは当然のことです。実際の格付けは信用調査委員会が、予測された格付けだけでなく格付けの微調整にとって重要な追加情報も考慮して判断したものだからです。<i></i></p><h2>おわりに<a name="23"></a></h2><p>MATLAB はバギングされた決定木以外にも多様な機械学習ツールを提供しており、それらも格付けに使用することができます。Statistics Toolbox には、識別分析や単純ベイズ分類器などの分類ツールが用意されています。Neural Networks Toolbox™ もあります。さらに、Database Toolbox や MATLAB の配布ツール (MATLAB Builder JA など) を使用すると、ここで説明したワークフローをより柔軟に独自の設定とニーズに合わせることができる可能性があります。</p><p>このデモでは、債務不履行確率は計算しませんでした。格付けでは、格付け移動履歴に基づいて債務不履行確率を計算するのが一般的です。詳細は、Financial Toolbox™ の <tt>transprob</tt> リファレンス ページを参照してください。</p><h2>参考文献<a name="24"></a></h2><p>[1] Altman, E., &quot;Financial Ratios, Discriminant Analysis and the Prediction of Corporate Bankruptcy,&quot; <i>Journal of Finance</i>, Vol. 23, No. 4, (Sep., 1968), pp.589-609.</p><p>[2] Basel Committee on Banking Supervision, &quot;Studies on the Validation of Internal Rating Systems,&quot; Bank for International Settlements (BIS), Working Papers No. 14, revised version, May 2005. Available at: <a href="http://www.bis.org/publ/bcbs_wp14.htm">http://www.bis.org/publ/bcbs_wp14.htm</a>.</p><p>[3] Basel Committee on Banking Supervision, &quot;International Convergence of Capital Measurement and Capital Standards:A Revised Framework,&quot; Bank for International Settlements (BIS), comprehensive version, June 2006. Available at: <a href="http://www.bis.org/publ/bcbsca.htm">http://www.bis.org/publ/bcbsca.htm</a>.</p><p>[4] Loeffler, G., and P. N. Posch, <i>Credit Risk Modeling Using Excel and VBA</i>, West Sussex, England:Wiley Finance, 2007.</p><p>[5] Merton, R., &quot;On the Pricing of Corporate Debt:The Risk Structure of Interest Rates,&quot; <i>Journal of Finance</i>, Vol. 29, No. 2, (May, 1974), pp. 449-70.449-70.</p><p class="footer">Copyright 2010 The MathWorks, Inc.<br>Published with MATLAB&reg; 7.13</p><p class="footer" id="trademarks">MATLAB and Simulink are registered trademarks of The MathWorks, Inc.  Please see <a href="http://www.mathworks.com/trademarks">www.mathworks.com/trademarks</a> for a list of other trademarks owned by The MathWorks, Inc.  Other product or brand names are trademarks or registered trademarks of their respective owners.</p></div><!-- ##### SOURCE BEGIN ##### %% Credit Rating by Bagging Decision Trees % % One of the fundamental tasks in credit risk management is to assign a % credit grade to a borrower. Grades are used to rank customers according % to their perceived creditworthiness: better grades mean less risky % customers; similar grades mean similar level of risk. Grades come in two % categories: credit ratings and credit scores. Credit ratings are a small % number of discrete classes, usually labeled with letters, such as 'AAA', % 'BB-', etc. Credit scores are numeric grades such as '640' or '720'. % Credit grades are one of the key elements in regulatory frameworks, such % as Basel II (see Basel Committee on Banking Supervision [3]). % % Assigning a credit grade involves analyzing information on the borrower. % If the borrower is an individual, information of interest could be the % individual's income, outstanding debt (mortgage, credit cards), household % size, residential status, etc. For corporate borrowers, one may consider % certain financial ratios (e.g., sales divided by total assets), industry, % etc. Here, we refer to these pieces of information about a borrower as % _features_ or _predictors_. Different institutions use different % predictors, and they may also have different rating classes or score % ranges to rank their customers. For relatively small loans offered to a % large market of potential borrowers (e.g., credit cards), it is common to % use credit scores, and the process of grading a borrower is usually % automated. For larger loans, accessible to small- to medium-sized % companies and larger corporations, credit ratings are usually used, and % the grading process may involve a combination of automated algorithms and % expert analysis. % % There are rating agencies that keep track of the creditworthiness of % companies. Yet, most banks develop an internal methodology to assign % credit grades for their customers. Rating a customer internally can be a % necessity if the customer has not been rated by a rating agency, but even % if a third-party rating exists, an internal rating offers a complementary % assessment of a customer's risk profile. % % This demo shows how MATLAB(R) can help with the automated stage of a % credit rating process. In particular, we take advantage of one of the % statistical learning tools readily available in Statistics Toolbox(TM), % a classification algorithm known as a _bagged decision tree_. % % We assume that historical information is available in the form of a data % set where each record contains the features of a borrower and the credit % rating that was assigned to it. These may be internal ratings, assigned % by a committee that followed policies and procedures already in place. % Alternatively, the ratings may come from a rating agency, whose ratings % are being used to "jump start" a new internal credit rating system. % % The existing historical data is the starting point of this demo, and it % is used to _train_ the bagged decision tree that will automate the % credit rating. In the vocabulary of statistical learning, % this training process falls in the category of % _supervised learning_. The classifier is then used to assign ratings to % new customers. In practice, these automated or _predicted_ ratings would % most likely be regarded as tentative, until a credit committee of experts % reviews them. The type of classifier we use in this demo can also % facilitate the revision of these ratings, because it provides a measure % of certainty for the predicted ratings, a _classification score_. % % In practice, one needs to train a classifier first, then use it to assign % a credit rating to new customers, and finally one also needs to _profile_ % or _evaluate the quality_ or accuracy of the classifier, a process also % known as _validation_ or _back-testing_. We discuss some readily % available back-testing tools in this demo, as well.  % Copyright 2010-2011 The MathWorks, Inc. % $Revision: 1.1.6.3 $   $Date: 2012/02/14 03:55:33 $  %% Loading the Existing Credit Rating Data % % We load the historical data from the comma-delimited text file % |CreditRating_Historical.dat|. We choose to work with text files for this demo, % but users with access to Database Toolbox(TM) can certainly load % this information directly from a database. % % The data set contains financial ratios, industry sector, and credit   % ratings for a list of corporate customers. This is simulated, not real % data. The first column is a customer ID. Then we have five columns of % financial ratios. These are the same ratios used in Altman's z-score (see % Altman [1]; see also Loeffler and Posch [4] for a related analysis). % % * Working capital / Total Assets (WC_TA) % % * Retained Earnings / Total Assets (RE_TA) % % * Earnings Before Interests and Taxes / Total Assets (EBIT_TA) % % * Market Value of Equity / Book Value of Total Debt (MVE_BVTD) % % * Sales / Total Assets (S_TA) % % Next, we have an industry sector label, an integer value ranging from 1 % to 12. The last column has the credit rating assigned to the customer. We % load the data into a |dataset| array.  creditDS = dataset('file','CreditRating_Historical.dat','delimiter',',');   %% % We copy the features into a matrix |X|, and the corresponding classes, % the ratings, into a vector |Y|. This is not a required step, since we % could access this information directly from the |dataset| array, but we % do it here to simplify some repeated function calls below. % % The features to be stored in the matrix |X| are the five financial % ratios, and the industry label. |Industry| is a categorical variable, % _nominal_ in fact, because there is no ordering in the industry sectors. % The response variable, the credit ratings, is also categorical, though % this is an _ordinal_ variable, because, by definition, ratings imply a % _ranking_ of creditworthiness. We can use this variable "as is" to train % our classifier. Here we choose to copy it into an _ordinal array_ because % this way the outputs come out in the natural order of the ratings and are % easier to read. The ordering of the ratings is established by the cell % array we pass as a third argument in the definition of |Y|. The credit % ratings can also be mapped into numeric values, which can be useful to % try alternative methods to analyze the data (e.g., regression). It is % always recommended to try different methods in practice.  X = [creditDS.WC_TA creditDS.RE_TA creditDS.EBIT_TA creditDS.MVE_BVTD...      creditDS.S_TA creditDS.Industry]; Y = ordinal(creditDS.Rating,[],{'AAA' 'AA' 'A' 'BBB' 'BB' 'B' 'CCC'});   %% % We use the predictors |X| and the response |Y| to fit a particular type % of classification ensemble called a _bagged decision tree_. "Bagging," in this % context, stands for "bootstrap aggregation." The methodology consists in % generating a number of sub-samples, or _bootstrap replicas_, from the % data set. These sub-samples are randomly generated, sampling with % replacement from the list of customers in the data set. For each replica, % a decision tree is grown. Each decision tree is a trained classifier on % its own, and could be used in isolation to classify new customers. The % predictions of two trees grown from two different bootstrap replicas may % be different, though. The ensemble _aggregates_ the  % predictions of all the decision trees that are grown for all the % bootstrap replicas. If the majority of the trees predict one particular % class for a new customer, it is reasonable to consider that prediction to % be more robust than the prediction of any single tree alone. Moreover, if % a different class is predicted by a smaller set of trees, that % information is useful, too. In fact, the proportion of trees that predict % different classes is the basis for the _classification scores_ that are % reported by the ensemble when classifying new data.  %% Constructing the Tree Bagger % % The first step to construct our classification ensemble will be to find a % good leaf size for the individual trees; here we try sizes of 1, 5 and % 10. (We recommend that you look at the |TreeBagger| reference page and % the "Regression and Classification by Bagging Decision Trees" section in % the User's Guide, both in Statistics Toolbox, to learn % more about this tool.) We start with a small number of trees, 25 only, % because we mostly want to compare the initial trend in the classification % error for different leaf sizes. For reproducibility and fair comparisons, % we reinitialize the random number generator, which is used to sample % with replacement from the data, each time we build a classifier.  leaf = [1 5 10]; nTrees = 25; rng(9876,'twister'); savedRng = rng; % save the current RNG settings  color = 'bgr'; for ii = 1:length(leaf)    % Reinitialize the random number generator, so that the    % random samples are the same for each leaf size    rng(savedRng);    % Create a bagged decision tree for each leaf size and plot out-of-bag    % error 'oobError'    b = TreeBagger(nTrees,X,Y,'oobpred','on','cat',6,'minleaf',leaf(ii));    plot(b.oobError,color(ii));    hold on; end xlabel('Number of grown trees'); ylabel('Out-of-bag classification error'); legend({'1', '5', '10'},'Location','NorthEast'); title('Classification Error for Different Leaf Sizes'); hold off;  %% % The errors are comparable for the three leaf-size options. We will % therefore work with a leaf size of 10, because it results in leaner trees % and more efficient computations. % % Note that we did not have to split the data into _training_ and _test_ % subsets. This is done internally, it is implicit in the sampling % procedure that underlies the method. At each bootstrap iteration, the % bootstrap replica is the training set, and any customers left out % ("out-of-bag") are used as test points to estimate the out-of-bag % classification error reported above. % % Next, we want to find out whether all the features are important for the % accuracy of our classifier. We do this by turning on the _feature % importance_ measure (|oobvarimp|), and plot the results to visually find % the most important features. We also try a larger number of trees now, % and store the classification error, for further comparisons below.  nTrees = 50; leaf = 10; rng(savedRng); b = TreeBagger(nTrees,X,Y,'oobvarimp','on','cat',6,'minleaf',leaf);  bar(b.OOBPermutedVarDeltaError); xlabel('Feature number'); ylabel('Out-of-bag feature importance'); title('Feature importance results');  oobErrorFullX = b.oobError;  %% % Features 2, 4 and 6 stand out from the rest. Feature 4, market value of % equity / book value of total debt (|MVE_BVTD|), is the most important % predictor for this data set. This ratio is closely related to the % predictors of creditworthiness in structural models, such as Merton's % model [5], where the value of the firm's equity is compared to its % outstanding debt to determine the default probability. % % Information on the industry sector, feature 6 (|Industry|), is also % relatively more important than other variables to assess the % creditworthiness of a firm for this data set. % % Although not as important as |MVE_BVTD|, feature 2, retained earnings / % total assets (|RE_TA|), stands out from the rest. There is a correlation % between retained earnings and the age of a firm (the longer a firm has % existed, the more earnings it can accumulate, in general), and in turn % the age of a firm is correlated to its creditworthiness (older firms tend % to be more likely to survive in tough times).  % % Let us fit a new classification ensemble using only predictors  % |RE_TA|, |MVE_BVTD|, and |Industry|. We compare its classification error % with the previous classifier, which uses all features.  X = [creditDS.RE_TA creditDS.MVE_BVTD creditDS.Industry];  rng(savedRng); b = TreeBagger(nTrees,X,Y,'oobpred','on','cat',3,'minleaf',leaf);  oobErrorX246 = b.oobError;  plot(oobErrorFullX,'b'); hold on; plot(oobErrorX246,'r'); xlabel('Number of grown trees'); ylabel('Out-of-bag classification error'); legend({'All features', 'Features 2, 4, 6'},'Location','NorthEast'); title('Classification Error for Different Sets of Predictors'); hold off;  %% % The accuracy of the classification does not deteriorate significantly % when we remove the features with relatively low importance (1, 3, and 5), % so we will use the more parsimonious classification ensemble for our % predictions. % % In this example, we have started with a set of six features only, and % used the feature importance measure of the classifier, and the % out-of-bag classification error as criteria to screen out three of the % variables. Feature selection can be a time consuming process when the % initial set of potential predictors contains dozens of variables. Besides % the tools we have used here (variable importance and a "visual" % comparison of out-of-bag errors), tools such as |sequentialfs| in % Statistics Toolbox can be helpful for these types of analyses. (See also % the demo "Selecting Features for Classifying High-dimensional Data," also % in Statistics Toolbox). However, in the end, a successful feature % selection process requires a combination of quantitative tools and an % analyst's judgement. % % For example, the variable importance measure we used % here is a ranking mechanism that estimates the relative impact of a % feature by measuring how much the predictive accuracy of the classifier % deteriorates when this feature's values are randomly permuted. The idea % is that when the feature in question adds little to the predictive power % of the classifier, using altered (in this case permuted) values should % not impact the classification results. Relevant information, on the other % hand, cannot be randomly swapped without degrading the predictions. Now, % if two highly correlated features are important, they will both rank high % in this analysis. In that case, keeping one of these features should % suffice for accurate classifications, but one would not know that from % the ranking results alone. One would have to check the correlations % separately, or use an expert's judgement. That is to say, tools like % variable importance or |sequentialfs| can greatly help for feature % selection, but an analyst's judgment is a key piece in this process.  %% % At this point, the classifier could be saved (e.g., |save classifier.mat % b|), to be loaded in a future session (|load classifier|) to classify new % customers. For efficiency, it is recommended to keep a compact version of % the classifier once the training process is finished.  b = b.compact;  %% Classifying New Data % % Here we use the previously constructed classification ensemble to assign % credit ratings to new customers. Because the ratings of existing % customers need to be reviewed, too, on a regular basis, especially when % their financial information has substantially changed, the data set could % also contain a list of existing customers under review. We start by % loading the new data.  newDS = dataset('file','CreditRating_NewCompanies.dat','delimiter',',');  %% % To predict the credit rating for this new data, we call the |predict| % method on the classifier. The method returns two arguments, the predicted % class and the classification score. We certainly want to get both output % arguments, since the classification scores contain information on how % certain the predicted ratings seem to be. We could copy variables % |RE_TA|, |MVE_BVTD| and |Industry| into a matrix |X|, as before, but % since we will make only one call to |predict|, we can skip this step and % use |newDS| directly.  [predClass,classifScore] = b.predict([newDS.RE_TA newDS.MVE_BVTD...    newDS.Industry]);  %% % At this point, we can create a report. Here we only display on the screen % a small report for the first three customers, for illustration purposes, % but MATLAB's deployment tools could greatly improve the workflow here. % For example, using MATLAB Builder(TM) JA, credit analysts could run this % classification remotely, using a web browser, and get a report, without % even having MATLAB on their desktops.  for i = 1:3    fprintf('Customer %d:\n',newDS.ID(i));    fprintf('   RE/TA    = %5.2f\n',newDS.RE_TA(i));    fprintf('   MVE/BVTD = %5.2f\n',newDS.MVE_BVTD(i));    fprintf('   Industry = %2d\n',newDS.Industry(i));    fprintf('   Predicted Rating : %s\n',predClass{i});    fprintf('   Classification score : \n');    for j = 1:length(b.ClassNames)       if (classifScore(i,j)>0)          fprintf('      %s : %5.4f \n',b.ClassNames{j},classifScore(i,j));       end    end end  %% % Keeping records of the predicted ratings and corresponding scores can be % useful for periodic assessments of the quality of the classifier. We % store this information here in the |dataset| array |predDS|.  predDS = dataset({newDS.ID,'ID'},{predClass,'PredRating'},...    {classifScore,'sAAA','sAA','sA','sBBB','sBB','sB','sCCC'});  %% % This information could be saved, for example, to a comma-delimited text file % |PredictedRatings.dat| using the command % %    export(predDS,'file','PredictedRatings.dat','delimiter',',') % % or written directly to a database using Database Toolbox.  %% Back-Testing: Profiling the Classification Process % % _Validation_ or _back-testing_ is the process of profiling or assessing % the quality of the credit ratings. There are many different measures and % tests related to this task (see, for example, Basel Committee on Banking % Supervision [2]). In this demo, we focus on the following two questions: % % * How accurate are the predicted ratings, as compared to the actual % ratings? Here "predicted ratings" refers to those obtained from the % automated classification process, and "actual ratings" to those assigned % by a credit committee that puts together the predicted ratings and their % classification scores, and other pieces of information, such as news and % the state of the economy to determine a final rating. % % * How well do the actual ratings rank customers according to their % creditworthiness? This is done in an _ex-post_ analysis performed, for % example, one year later, when it is known which companies defaulted % during the year. % % The file |CreditRating_ExPost.dat| contains "follow up" data on the same % companies considered in the previous section. It contains the actual % ratings that the committee assigned to these companies, as well as a % "default flag" that indicates whether the corresponding company defaulted % within one year of the rating process (if 1) or not (if 0).  exPostDS = dataset('file','CreditRating_ExPost.dat','delimiter',',');  %% % If this were a new MATLAB session, besides reading the follow up data % from |CreditRating_ExPost.dat| we would need to load the predicted ratings % information stored in |predDS|, for example, with the command % %    predDS = dataset('file','PredictedRatings.dat','delimiter',',')  %% % *Comparing predicted ratings vs. actual ratings.* The rationale to train % an automated classifier is to expedite the work of the credit committee. % The more accurate the predicted ratings are, the less time the committee % has to spend reviewing the predicted ratings. So it is conceivable that % the committee wants to have regular checks on how closely the predicted % ratings match the final ratings they assign, and to recommend re-training % the automated classifier (and maybe include new features, for example) if % the mismatch seems concerning. % % The first tool we can use to compare predicted vs. actual ratings is a % _confusion matrix_, readily available in Statistics Toolbox:  C = confusionmat(exPostDS.Rating,predDS.PredRating,...    'order',{'AAA' 'AA' 'A' 'BBB' 'BB' 'B' 'CCC'})  %% % The rows in |C| correspond to the actual ratings, and the columns to the % predicted ratings. The amount in the position |(i,j)| in this matrix % indicates how many customers received an actual rating |i| and were % predicted as rating |j|. For example, position |(3,2)| tells us how many % customers received a rating of 'A' by the credit committee, but were % predicted as 'AA' with the automated classifier. One can also present % this matrix in percentage form with a simple transformation:  Cperc = diag(sum(C,2))\C  %% % Good agreement between the predicted and the actual ratings would result % in values in the main diagonal that dominate the rest of the values in a % row, ideally values close to 1. In this case, we actually see an % important disagreement for 'B,' since about half of the customers that % were rated as 'B' by the credit committee had been predicted as 'BB' by % the automated classifier. On the other hand, it is good to see that % ratings differ in at most one notch in most cases, with the only % exception of 'BBB.' % % A confusion matrix could also be used to compare the internal ratings % assigned by the institution against third-party ratings; this is often % done in practice. % % For each specific rating, we can compute yet another measure of agreement % between predicted and actual ratings. We can build a _Receiver Operating % Characteristic (ROC) curve_ using the |perfcurve| function from % Statistics Toolbox, and check the _area under the curve (AUC)_. The % |perfcurve| function takes as an argument the actual ratings, which are % our benchmark, the standard we are comparing against, and the 'BBB' % classification scores determined by the automated process. Let us % build a ROC and calculate the AUC for rating 'BBB' in our example.  [xVal,yVal,~,auc] = perfcurve(exPostDS.Rating,predDS.sBBB,'BBB'); plot(xVal,yVal); xlabel('False positive rate'); ylabel('True positive rate'); text(0.5,0.25,strcat('AUC=',num2str(auc)),'EdgeColor','k'); title('ROC curve BBB, predicted vs. actual rating');  %% % Here is an explanation of how the ROC is built. Recall that for each % customer the automated classifier returns a classification score for each % of the credit ratings, in particular, for 'BBB,' which can be interpreted % as how likely it is that this particular customer should be rated 'BBB.' % In order to build the ROC curve, one needs to vary the % _classification threshold_. That is, the minimum score to classify % a customer as 'BBB.' In other words, if the threshold is |t|, we only % classify customers as 'BBB' if their 'BBB' score is greater than or equal % to |t|. For example, suppose that company _XYZ_ had a 'BBB' score of % 0.87. If the actual rating of _XYZ_ (the information in % |exPostDS.Rating|) is 'BBB,' then _XYZ_ would be correctly classified as % 'BBB' for any threshold of up to 0.87. This would be a _true positive_, % and it would increase what is call the _sensitivity_ of the classifier. % For any threshold greater than 0.87, this company would not receive a % 'BBB' rating, and we would have a _false negative_ case. To complete the % description, suppose now that _XYZ_'s actual rating is 'BB.' Then it % would be correctly rejected as a 'BBB' for thresholds of more than 0.87, % becoming a _true negative_, and thus increasing the so called % _specificity_ of the classifier. However, for thresholds of up to 0.87, % it would become a _false positive_ (it would be classified as 'BBB,' when % it actually is a 'BB'). The ROC curve is constructed by plotting the % proportion of true positives (sensitivity), versus false positives % (1-specificity), as the threshold varies from 0 to 1. % % The AUC, as its name indicates, is the area under the ROC curve. The % closer the AUC is to 1, the more accurate the classifier (a perfect % classifier would have an AUC of 1). In this example, the AUC seems high % enough, but it would be up to the committee to decide which level of AUC % for the ratings should trigger a recommendation to improve the automated % classifier.  %% % *Comparing actual ratings vs. defaults in the following year.* A common % tool used to assess the ranking of customers implicit in the credit % ratings is the _Cumulative Accuracy Profile (CAP)_, and the associated % _accuracy ratio_ measure. The idea is to measure the relationship between % the credit ratings assigned and the number of defaults observed in the % following year. One would expect that fewer defaults are observed for % better rating classes. If the default rate were the same for all ratings, % the rating system would be no different from a naive (and useless) % classification system in which customers were randomly assigned a rating, % independently of their creditworthiness. % % It is not hard to see that the |perfcurve| function can also be used to % construct the CAP. The standard we compare against is not a rating, as % before, but the default flag that we loaded from the |CreditRating_ExPost.dat| % file. The score we use is a "dummy score" that indicates the ranking in % creditworthiness implicit in the list of ratings. The dummy score only % needs to satisfy that better ratings get lower dummy scores (they are % "less likely to have a default flag of 1"), and that any two customers % with the same rating get the same dummy score. A default probability % could be passed as a score, of course, but we do not have default % probabilities here, and in fact _we do not need to have estimates of the % default probabilities to construct the CAP_, because we are not % validating default probabilities. All we are assessing with this tool is % how well the ratings _rank_ customers according to their % creditworthiness. % % Usually, the CAP of the rating system under consideration is plotted % together with the CAP of the "perfect rating system." The latter is a % hypothetical credit rating system for which the lowest rating includes % all the defaulters, and no other customers. The area under this perfect % curve is the maximum possible AUC attainable by a rating system. By % convention, the AUC is adjusted for CAPs to subtract the area under the % _naive system_'s CAP, that is, the CAP of the system that randomly % assigns ratings to customers. The naive system's CAP is simply a straight % line from the origin to (1,1), with an AUC of 0.5. The _accuracy ratio_ % for a rating system is then defined as the ratio of the adjusted AUC (AUC % of the system in consideration minus AUC of the naive system) to the % maximum accuracy (AUC of the perfect system minus AUC of the naive % system).  ratingsList = {'AAA' 'AA' 'A' 'BBB' 'BB' 'B' 'CCC'}; Nratings = length(ratingsList); dummyDelta = 1/(Nratings+1); dummyRank = linspace(dummyDelta,1-dummyDelta,Nratings)';  D = exPostDS.Def_tplus1; fracTotDef = sum(D)/length(D); maxAcc = 0.5 - 0.5 * fracTotDef;  R = double(ordinal(exPostDS.Rating,[],ratingsList)); S = dummyRank(R); [xVal,yVal,~,auc] = perfcurve(D,S,1);  accRatio = (auc-0.5)/maxAcc; fprintf('Accuracy ratio for actual ratings: %5.3f\n',accRatio);  xPerfect(1) = 0; xPerfect(2) = fracTotDef; xPerfect(3) = 1; yPerfect(1) = 0; yPerfect(2) = 1; yPerfect(3) = 1; xNaive(1) = 0; xNaive(2) = 1; yNaive(1) = 0; yNaive(2) = 1;  plot(xPerfect,yPerfect,'REPLACE_WITH_DASH_DASHk',xVal,yVal,'b',xNaive,yNaive,'-.k'); xlabel('Fraction of all companies'); ylabel('Fraction of defaulted companies'); title('Cumulative Accuracy Profile'); legend({'Perfect','Actual','Naive'},'Location','SouthEast'); text(xVal(2)+0.01,yVal(2)-0.01,'CCC') text(xVal(3)+0.01,yVal(3)-0.02,'B') text(xVal(4)+0.01,yVal(4)-0.03,'BB')  %% % The key to reading the information of the CAP is in the "kinks," labeled % in the plot for ratings 'CCC,' 'B,' and 'BB.' For example, the second % kink is associated with the second lowest rating, 'B,' and it is located % at (0.097, 0.714). This means that 9.7% of the customers were ranked 'B' % _or lower_, and they account for 71.4% of the defaults observed. % % In general, the accuracy ratio should be treated as a relative, rather % than an absolute measure. For example, we can add the CAP of the % predicted ratings in the same plot, and compute its accuracy ratio to % compare it with the accuracy ratio of the actual ratings.  Rpred = double(ordinal(predDS.PredRating,[],ratingsList)); Spred = dummyRank(Rpred); [xValPred,yValPred,~,aucPred] = perfcurve(D,Spred,1);  accRatioPred = (aucPred-0.5)/maxAcc; fprintf('Accuracy ratio for predicted ratings: %5.3f\n',accRatioPred);  plot(xPerfect,yPerfect,'REPLACE_WITH_DASH_DASHk',xVal,yVal,'b',xNaive,yNaive,'-.k',...    xValPred,yValPred,':r'); xlabel('Fraction of all companies'); ylabel('Fraction of defaulted companies'); title('Cumulative Accuracy Profile'); legend({'Perfect','Actual','Naive','Predicted'},'Location','SouthEast');  %% % The accuracy ratio of the predicted rating is smaller, and its CAP is % mostly below the CAP of the actual rating. This is reasonable, since the % actual ratings are assigned by the credit committees that take into % consideration the predicted ratings _and_ extra information that can be % important to fine-tune the ratings.  %% Final Remarks % % MATLAB offers a wide range of machine learning tools, besides bagged % decision trees, that can be used in the context of credit rating. In % Statistics Toolbox you can find classification tools such as discriminant % analysis and naive Bayes classifiers. MATLAB also offers Neural Networks % Toolbox(TM). Also, Database Toolbox and MATLAB's deployment tools % such as MATLAB Builder JA may provide you with more flexibility to adapt % the workflow presented here to your own preferences and needs. % % No probabilities of default have been computed in this demo. For credit % ratings, the probabilities of default are usually computed based on % credit-rating migration history. See the |transprob| reference page in % Financial Toolbox(TM) for more information.  %% Bibliography % % [1] Altman, E., "Financial Ratios, Discriminant Analysis and the % Prediction of Corporate Bankruptcy," _Journal of Finance_, Vol. 23, No. % 4, (Sep., 1968), pp. 589-609. % % [2] Basel Committee on Banking Supervision, "Studies on the Validation of % Internal Rating Systems," Bank for International Settlements (BIS), % Working Papers No. 14, revised version, May 2005. Available at: % http://www.bis.org/publ/bcbs_wp14.htm. % % [3] Basel Committee on Banking Supervision, "International Convergence of % Capital Measurement and Capital Standards: A Revised Framework," Bank for % International Settlements (BIS), comprehensive version, June 2006. % Available at: http://www.bis.org/publ/bcbsca.htm. % % [4] Loeffler, G., and P. N. Posch, _Credit Risk Modeling Using Excel and % VBA_, West Sussex, England: Wiley Finance, 2007. % % [5] Merton, R., "On the Pricing of Corporate Debt: The Risk Structure of % Interest Rates," _Journal of Finance_, Vol. 29, No. 2, (May, 1974), pp. % 449-70. %  displayEndOfDemoMessage(mfilename)  ##### SOURCE END ##### --></body></html>