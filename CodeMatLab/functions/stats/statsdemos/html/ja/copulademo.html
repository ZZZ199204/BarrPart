
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- This HTML was auto-generated from MATLAB code. To make changes, update the MATLAB code and republish this document.       --><title>コピュラを使用した従属確率変数のシミュレーション</title><meta name="generator" content="MATLAB 7.14"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2011-11-14"><meta name="DC.source" content="copulademo.m"><link rel="stylesheet" type="text/css" href="../../../../matlab/helptools/private/style.css"><link rel="stylesheet" type="text/css" href="../../../../matlab/helptools/private/style_ja_JP.css"></head><body><div class="header"><div class="left"><a href="matlab:edit copulademo">エディターで copulademo.m を開く</a></div><div class="right"><a href="matlab:echodemo copulademo">コマンド ウィンドウで実行</a></div></div><div class="content"><h1>コピュラを使用した従属確率変数のシミュレーション</h1><!--introduction--><p>この例では、コピュラを使用して、変数間に複雑な関係がある多変量分布や、個々の変数が異なる分布から成る多変量分布からデータを生成する方法を示します。</p><p>MATLAB&reg; は、ランダムな入力やノイズを組み込むシミュレーションを実行するのに理想的なツールです。Statistics Toolbox™ には、多くの一般的な一変量分布に従って一連の乱数データを作成する関数が用意されています。また、多変量正規分布や多変量 t 分布などの多変量分布から乱数データを生成する関数もあります。ただし、変数間に複雑な関係がある多変量分布や、個々の変数が異なる分布から成る多変量分布からデータを生成するための方法は組み込まれていません。</p><p>最近、コピュラはシミュレーション モデルで一般的になってきました。コピュラは、変数間の依存関係を記述する関数であり、相関がある多変量データをモデル化する分布を作成する方法を提供します。コピュラを使うと、データ解析者は一変量周辺分布を指定することにより多変量分布を作成し、次に、特定のコピュラを選択し、変数間の相関構造を与えることができます。より高次元の分布と同様に、2 変量の分布も可能です。この例では、Statistics Toolbox を使用して MATLAB で従属多変量乱数データを生成するためのコピュラの使用方法について説明します。</p><!--/introduction--><h2>目次</h2><div><ul><li><a href="#1">シミュレーション入力間の依存関係</a></li><li><a href="#8">依存する 2 変量分布を作成するための一般的な方法</a></li><li><a href="#13">順位相関係数</a></li><li><a href="#15">コピュラ分布</a></li><li><a href="#17">t コピュラ</a></li><li><a href="#20">より高次元のコピュラ</a></li><li><a href="#23">コピュラと経験的な周辺分布</a></li></ul></div><h2>シミュレーション入力間の依存関係<a name="1"></a></h2><p>モンテ・カルロ シミュレーションの設計で決めることの 1 つは、ランダムな入力に対する確率分布の選択です。個々の変数それぞれに対して分布を選択することは、多くの場合簡単ですが、入力の間にどのような依存関係があるかを判断することは、簡単ではないことがあります。理想的には、シミュレーションへの入力データは、モデル化される実際の量間の依存関係について既知であることを、反映しなければなりません。ただし、シミュレーションにおいて任意の依存関係について基にするべき情報がほとんどなかったり、あるいはまったくないことがあります。そのような場合、モデルの感度を判断するために、さまざまな可能性を試すことが有効です。</p><p>ただし、分布が標準多変量分布ではない場合、依存関係をもつランダムな入力を実際に作成することは、難しくなることがあります。さらに、標準多変量分布のいくつかでは、依存関係に非常に制限のあるタイプに限りモデル化できます。入力を独立にすることは、常に可能であり容易な選択ですが必ずしも目的にかなったものではなく、誤った結論を導く可能性があります。</p><p>たとえば、財務リスクのモンテ・カルロ シミュレーションには、保険損失のさまざまな原因を表すランダムな入力がある場合があります。これらの入力は、対数正規分布の確率変数としてモデル化されることがあります。当然疑問となるのは、これら 2 つの入力間の依存関係がシミュレーションの結果にどのように影響するかということです。実際、同じランダムな条件が両原因に影響することが、実際のデータからわかることがあります。すなわち、シミュレーションにおいて、誤った結論につながる可能性を低減することができます。</p><p>対数正規分布を示す独立した確率変数のシミュレーションは自明です。最も簡単な方法は、関数 <tt>lognrnd</tt> を使用することです。ここで、関数 <tt>mvnrnd</tt> を使用して、n 組の独立した正規確率変数を生成してから、それらを累乗します。ここで使われている共分散行列は対角形であること、つまり Z の列間に独立性があることに注意してください。</p><pre class="codeinput">n = 1000;
sigma = .5;
SigmaInd = sigma.^2 .* [1 0; 0 1]
</pre><pre class="codeoutput">
SigmaInd =

    0.2500         0
         0    0.2500

</pre><pre class="codeinput">ZInd = mvnrnd([0 0], SigmaInd, n);
XInd = exp(ZInd);
plot(XInd(:,1),XInd(:,2),<span class="string">'.'</span>); axis <span class="string">equal</span>; axis([0 5 0 5]);
xlabel(<span class="string">'X1'</span>); ylabel(<span class="string">'X2'</span>);
</pre><img vspace="5" hspace="5" src="../copulademo_01.png" alt=""> <p>相関がある 2 変量対数正規確率変数は、非ゼロの非対角項をもつ共分散行列を使って、生成することも容易です。</p><pre class="codeinput">rho = .7;
SigmaDep = sigma.^2 .* [1 rho; rho 1]
</pre><pre class="codeoutput">
SigmaDep =

    0.2500    0.1750
    0.1750    0.2500

</pre><pre class="codeinput">ZDep = mvnrnd([0 0], SigmaDep, n);
XDep = exp(ZDep);
</pre><p>さらに散布図を表示することで、これら 2 つの 2 変量分布間の違いがわかります。</p><pre class="codeinput">plot(XDep(:,1),XDep(:,2),<span class="string">'.'</span>); axis <span class="string">equal</span>; axis([0 5 0 5]);
xlabel(<span class="string">'X1'</span>); ylabel(<span class="string">'X2'</span>);
</pre><img vspace="5" hspace="5" src="../copulademo_02.png" alt=""> <p>X2 の大きな値 (小さな値の場合も同様) に対して X1 の値が大きい場合、2 番目のデータセットに相関の傾向があることは明らかです。この依存は、基本的な 2 変量正規分布の相関関係パラメーター rho によって決まります。シミュレーションから導き出された結論は、依存関係をもつ X1 と X2 が生成されたかどうかでかなり異なります。</p><p>この場合、2 変量対数正規分布を利用することができます。周辺分布が &quot;異なる&quot; 対数正規分布である場合は、より高次元に容易に一般化されます。<i></i>他の多変量分布も考えられますが、たとえば、多変量 t 分布とディリクレ分布はそれぞれ、t 分布とベータ分布する従属確率変数をシミュレートするために使用されます。しかし、簡単な多変量分布は多くはなく、境界がすべて同じ族になる (あるいは、厳密に同じ分布にもなる) 場合に、適用が限られます。これは、多くの状況で実質的な制限となることがあります。</p><h2>依存する 2 変量分布を作成するための一般的な方法<a name="8"></a></h2><p>上記の 2 変量対数正規分布を作成する構造は簡単ですが、これはより一般に適用できる方法を説明するのに役立ちます。まず、2 変量正規分布からの値の組を生成します。これら 2 変量には、統計的な依存関係があり、それぞれは、正規周辺分布をもちます。次に、周辺分布を対数正規分布に変更して、それぞれの変数に変換 (指数関数) を別々に適用します。変換された変数には、やはり統計的な依存関係があります。</p><p>適切な変換が見つかれば、この方法は一般化されて、他の周辺分布をもつ、2 変量の従属しているランダムなベクトルを作成できます。実際、累乗法のみを使う場合程簡単ではありませんが、そのような変換を作成する一般的な方法があります。</p><p>定義では、ここで PHI と示される正規累積分布関数 (CDF) を、標準の正規確率変数に適用すると、区間 [0, 1] 上で一様である確率変数になります。これは、Z が標準正規分布をもつ場合、U = PHI(Z) の累積分布関数が以下のようになることでわかります。</p><pre>  Pr{U &lt;= u0} = Pr{PHI(Z) &lt;= u0} = Pr{Z &lt;= PHI^(-1)(u0)} = u0,</pre><p>さらに、これは U(0,1) の確率変数の累積分布関数です。シミュレーションによって得られた正規分布と変換された値のヒストグラムは、以下の事実を示します。</p><pre class="codeinput">n = 1000;
z = normrnd(0,1,n,1);
hist(z,-3.75:.5:3.75); xlim([-4 4]);
title(<span class="string">'1000 Simulated N(0,1) Random Values'</span>);
xlabel(<span class="string">'Z'</span>); ylabel(<span class="string">'Frequency'</span>);
</pre><img vspace="5" hspace="5" src="../copulademo_03.png" alt=""> <pre class="codeinput">u = normcdf(z);
hist(u,.05:.1:.95);
title(<span class="string">'1000 Simulated N(0,1) Values Transformed to U(0,1)'</span>);
xlabel(<span class="string">'U'</span>); ylabel(<span class="string">'Frequency'</span>);
</pre><img vspace="5" hspace="5" src="../copulademo_04.png" alt=""> <p>一変数の乱数生成の理論によると、U(0,1) に従う確率変数に、任意の分布 F の累積分布逆関数を適用すると、正確に F 分布する確率変数になります。これは逆関数法と呼ばれています。証明は、以前のケースに対する前述の証明とは本質的に逆になります。次のヒストグラムは、ガンマ分布への変換を示しています。</p><pre class="codeinput">x = gaminv(u,2,1);
hist(x,.25:.5:9.75);
title(<span class="string">'1000 Simulated N(0,1) Values Transformed to Gamma(2,1)'</span>);
xlabel(<span class="string">'X'</span>); ylabel(<span class="string">'Frequency'</span>);
</pre><img vspace="5" hspace="5" src="../copulademo_05.png" alt=""> <p>この 2 ステップの変換を 2 変量標準正規分布の各変数に適用して、任意の周辺分布をもつ従属確率変数を作成できます。変換は各成分に別々に作用するので、2 つの結果の確率変数は同じ周辺分布にはなりません。変換は、次の式で定義されます。</p><pre>  Z = [Z1 Z2] ~ N([0 0],[1 rho; rho 1])
  U = [PHI(Z1) PHI(Z2)]
  X = [G1(U1) G2(U2)]</pre><p>ここで、G1 と G2 は、2 つの可能な異なる分布の累積分布逆関数です。たとえば、t(5) 分布と Gamma(2,1) 周辺分布をもつ 2 変数分布からランダムなベクトルを生成します。</p><pre class="codeinput">n = 1000;
rho = .7;
Z = mvnrnd([0 0], [1 rho; rho 1], n);
U = normcdf(Z);
X = [gaminv(U(:,1),2,1) tinv(U(:,2),5)];
</pre><p>このプロットは、周辺分布と依存関係の両方を示すために、スキャター プロットと同時にヒストグラムも示します。</p><pre class="codeinput">[n1,ctr1] = hist(X(:,1),20);
[n2,ctr2] = hist(X(:,2),20);
subplot(2,2,2); plot(X(:,1),X(:,2),<span class="string">'.'</span>); axis([0 12 -8 8]); h1 = gca;
title(<span class="string">'1000 Simulated Dependent t and Gamma Values'</span>);
xlabel(<span class="string">'X1 ~ Gamma(2,1)'</span>); ylabel(<span class="string">'X2 ~ t(5)'</span>);
subplot(2,2,4); bar(ctr1,-n1,1); axis([0 12 -max(n1)*1.1 0]); axis(<span class="string">'off'</span>); h2 = gca;
subplot(2,2,1); barh(ctr2,-n2,1); axis([-max(n2)*1.1 0 -8 8]); axis(<span class="string">'off'</span>); h3 = gca;
set(h1,<span class="string">'Position'</span>,[0.35 0.35 0.55 0.55]);
set(h2,<span class="string">'Position'</span>,[.35 .1 .55 .15]);
set(h3,<span class="string">'Position'</span>,[.1 .35 .15 .55]);
colormap([.8 .8 1]);
</pre><img vspace="5" hspace="5" src="../copulademo_06.png" alt=""> <h2>順位相関係数<a name="13"></a></h2><p>この構造の X1 と X2 間の依存は、基本的な 2 変量正規分布の相関関係パラメーター rho によって決まります。ただし、X1 と X2 の線形相関が rho であるということは成り立ちません。<i></i>たとえば、オリジナルの対数正規分布の場合、その相関の閉形式は次のようになります。</p><pre>  cor(X1,X2) = (exp(rho.*sigma.^2) - 1) ./ (exp(sigma.^2) - 1)</pre><p>rho が 1 ではない場合は、rho より厳密に小さくなります。上記のガンマ/t 分布のような、より一般的な場合には、X1 と X2 の線形相関は、rho での表現が困難または不可能です。しかし、シミュレーションを使用すると、同じ効果が起こることを示すことができます。</p><p>これは、線形相関係数は確率変数間の &quot;線形&quot; の依存関係を表すためであり、非線形変換がこれらの確率変数に適用されるときに、線形相関は保存されません。<i></i>代わりに、ケンドールの tau またはスピアマンの rho などの順位相関係数は、より適切です。</p><p>概略を説明すると、これらの順位相関は、他の大きい値 (または小さい値) に関連する 1 つの確率変数の大きい値 (または小さい値) に対して、順位を測定します。しかし、線形相関係数とは違い、これらは、順位についての関連のみを測ります。その結果、順位相関は単調な変換では保存されます。特に、前述の transformation 法は、順位相関を保存します。したがって、2 変量の正規分布 Z の順位相関を正確に知れば、変換された最終の確率変数 X の順位相関が決まります。線形相関係数は、基になっている 2 変量正規分布をパラメーター化するために必要になりますが、ケンドールの順位相関係数またはスピアマンの順位相関係数 rho の方が、確率変数の依存関係を説明するよりも効果的です。なぜなら、これらは周辺分布の選択に依存しないからです。</p><p>2 変量正規分布の場合、ケンドールの tau またはスピアマンの rho と線形相関係数 rho には次の簡単な 1 対 1 写像があります。</p><pre>  tau   = (2/pi)*arcsin(rho)     or   rho = sin(tau*pi/2)
  rho_s = (6/pi)*arcsin(rho/2)   or   rho = 2*sin(rho_s*pi/6)</pre><pre class="codeinput">subplot(1,1,1);
rho = -1:.01:1;
tau = 2.*asin(rho)./pi;
rho_s = 6.*asin(rho./2)./pi;
plot(rho,tau,<span class="string">'-'</span>, rho,rho_s,<span class="string">'-'</span>, [-1 1],[-1 1],<span class="string">'k:'</span>); axis([-1 1 -1 1]);
xlabel(<span class="string">'rho'</span>); ylabel(<span class="string">'Rank correlation coefficient'</span>);
legend(<span class="string">'Kendall''s tau'</span>, <span class="string">'Spearman''s rho_s'</span>, <span class="string">'location'</span>,<span class="string">'northwest'</span>);
</pre><img vspace="5" hspace="5" src="../copulademo_07.png" alt=""> <p>したがって、Z1 と Z2 の線形相関に対して、それらの周辺分布にかかわらず、正しい rho パラメーター値を選択することによって、X1 と X2 の指定する順位相関を作成することは容易です。</p><p>多変量正規分布の場合、スピアマンの順位相関は線形相関とほとんど同じです。しかし、最後に確率変数に変換すると、このことは成り立ちません。</p><h2>コピュラ<a name="15"></a></h2><p>上記の説明の最初のステップは、コピュラ、明確にはガウス･コピュラとして知られるものを定義します。2 変量コピュラは、2 つの確率変数の確率分布であり、それぞれの周辺分布は一様です。これら 2 つの変数は、完全に独立であるか、確定的に関連 (例: U2 = U1) があるか、あるいはこれらの中間的なものです。2 変量ガウス･コピュラの族は、線形相関行列 Rho = [1 rho; rho 1] によってパラメーター化されます。U1 と U2 は、rho が &plusmn;1 に近づくと線形依存性に近づき、rho がゼロに近づくと完全な独立に近づきます。</p><p>さまざまなレベルの rho に対してシミュレートされた乱数値のスキャター プロットは、ガウス・コピュラのさまざまな可能性の範囲を示しています。</p><pre class="codeinput">n = 500;
Z = mvnrnd([0 0], [1 .8; .8 1], n);
U = normcdf(Z,0,1);
subplot(2,2,1); plot(U(:,1),U(:,2),<span class="string">'.'</span>);
title(<span class="string">'rho = 0.8'</span>); xlabel(<span class="string">'U1'</span>); ylabel(<span class="string">'U2'</span>);
Z = mvnrnd([0 0], [1 .1; .1 1], n);
U = normcdf(Z,0,1);
subplot(2,2,2); plot(U(:,1),U(:,2),<span class="string">'.'</span>);
title(<span class="string">'rho = 0.1'</span>); xlabel(<span class="string">'U1'</span>); ylabel(<span class="string">'U2'</span>);
Z = mvnrnd([0 0], [1 -.1; -.1 1], n);
U = normcdf(Z,0,1);
subplot(2,2,3); plot(U(:,1),U(:,2),<span class="string">'.'</span>);
title(<span class="string">'rho = -0.1'</span>); xlabel(<span class="string">'U1'</span>); ylabel(<span class="string">'U2'</span>);
Z = mvnrnd([0 0], [1 -.8; -.8 1], n);
U = normcdf(Z,0,1);
subplot(2,2,4); plot(U(:,1),U(:,2),<span class="string">'.'</span>);
title(<span class="string">'rho = -0.8'</span>); xlabel(<span class="string">'U1'</span>); ylabel(<span class="string">'U2'</span>);
</pre><img vspace="5" hspace="5" src="../copulademo_08.png" alt=""> <p>U1 と U2 の間の依存関係は、X1 = G(U1) と X2 = G(U2) の周辺分布とは完全に別です。X1 と X2 は、任意の周辺分布でも与えられ、さらに同じ順位相関をもちます。<i></i>これは、コピュラの主な利点の 1 つです。依存関係と周辺分布をこのように別々に特定することが可能になります。</p><h2>t コピュラ<a name="17"></a></h2><p>さまざまなコピュラ族は、2 変量 t 分布から始め、対応する t 累積分布関数を使って変換することによって作成できます。2 変量 t 分布は、線形相関行列 Rho と自由度 nu でパラメーター化されます。こうして、たとえば、それぞれ自由度 1 と 5 をもつ多変量 t 分布に基づき、t(1) コピュラまたは t(5) コピュラを記述できます。</p><p>さまざまなレベルの rho に対してシミュレートされた乱数値のスキャター プロットは、t(1) コピュラのさまざまな可能性の範囲を示しています。</p><pre class="codeinput">n = 500;
nu = 1;
T = mvtrnd([1 .8; .8 1], nu, n);
U = tcdf(T,nu);
subplot(2,2,1); plot(U(:,1),U(:,2),<span class="string">'.'</span>);
title(<span class="string">'rho = 0.8'</span>); xlabel(<span class="string">'U1'</span>); ylabel(<span class="string">'U2'</span>);
T = mvtrnd([1 .1; .1 1], nu, n);
U = tcdf(T,nu);
subplot(2,2,2); plot(U(:,1),U(:,2),<span class="string">'.'</span>);
title(<span class="string">'rho = 0.1'</span>); xlabel(<span class="string">'U1'</span>); ylabel(<span class="string">'U2'</span>);
T = mvtrnd([1 -.1; -.1 1], nu, n);
U = tcdf(T,nu);
subplot(2,2,3); plot(U(:,1),U(:,2),<span class="string">'.'</span>);
title(<span class="string">'rho = -0.1'</span>); xlabel(<span class="string">'U1'</span>); ylabel(<span class="string">'U2'</span>);
T = mvtrnd([1 -.8; -.8 1], nu, n);
U = tcdf(T,nu);
subplot(2,2,4); plot(U(:,1),U(:,2),<span class="string">'.'</span>);
title(<span class="string">'rho = -0.8'</span>); xlabel(<span class="string">'U1'</span>); ylabel(<span class="string">'U2'</span>);
</pre><img vspace="5" hspace="5" src="../copulademo_09.png" alt=""> <p>t コピュラは、ちょうどガウス･コピュラがもつように、U1 と U2 に対する一様な周辺分布をもちます。t コピュラの成分間の順位相関 tau または rho_s は、ガウス･コピュラの rho とも同じ関数です。しかし、これらのプロットが示すように、t(1) コピュラは、これらの成分が同じ順位相関をもつ場合でも、ガウス･コピュラとはかなり異なります。この違いは、それらの依存関係の構造にあります。予想どおり、自由度パラメーター nu が大きくなるにつれて、t(nu) コピュラは対応するガウス･コピュラに近づきます。</p><p>ガウス･コピュラと同様に、任意の周辺分布は t コピュラに適用できます。たとえば、自由度 1 の t コピュラを使って、Gam(2,1) と t(5) 周辺分布をもつ 2 変量分布から、ランダムなベクトルを再び生成できます。</p><pre class="codeinput">subplot(1,1,1);
n = 1000;
rho = .7;
nu = 1;
T = mvtrnd([1 rho; rho 1], nu, n);
U = tcdf(T,nu);
X = [gaminv(U(:,1),2,1) tinv(U(:,2),5)];

[n1,ctr1] = hist(X(:,1),20);
[n2,ctr2] = hist(X(:,2),20);
subplot(2,2,2); plot(X(:,1),X(:,2),<span class="string">'.'</span>); axis([0 15 -10 10]); h1 = gca;
title(<span class="string">'1000 Simulated Dependent t and Gamma Values'</span>);
xlabel(<span class="string">'X1 ~ Gamma(2,1)'</span>); ylabel(<span class="string">'X2 ~ t(5)'</span>);
subplot(2,2,4); bar(ctr1,-n1,1); axis([0 15 -max(n1)*1.1 0]); axis(<span class="string">'off'</span>); h2 = gca;
subplot(2,2,1); barh(ctr2,-n2,1); axis([-max(n2)*1.1 0 -10 10]); axis(<span class="string">'off'</span>); h3 = gca;
set(h1,<span class="string">'Position'</span>,[0.35 0.35 0.55 0.55]);
set(h2,<span class="string">'Position'</span>,[.35 .1 .55 .15]);
set(h3,<span class="string">'Position'</span>,[.1 .35 .15 .55]);
colormap([.8 .8 1]);
</pre><img vspace="5" hspace="5" src="../copulademo_10.png" alt=""> <p>ガウス･コピュラに基づき以前に作成された 2 変量のガンマ/t 分布と比較して、t(1) コピュラに基づきここで作成された分布は、同じ周辺分布と変数間の同じ順位相関をもちますが、非常に異なる依存構造をもちます。これは、多変量分布は、周辺分布またはそれらの相関によって一意に定義されないということを示しています。アプリケーションで特定のコピュラの選択は、実際の観測されるデータに基づくことがあります。あるいは、入力の分布に対するシミュレーションの結果の感度を決める方法として、別のコピュラが使用されることもあります。</p><h2>より高次元のコピュラ<a name="20"></a></h2><p>ガウス･コピュラと t コピュラは、楕円コピュラとして知られています。楕円コピュラをより高次元に一般化することは容易です。たとえば、以下のように、ガウス･コピュラを使用して、Gamma(2,1)、Beta(2,2)、t(5) 周辺分布をもつ 3 変量分布からのデータをシミュレートします。</p><pre class="codeinput">subplot(1,1,1);
n = 1000;
Rho = [1 .4 .2; .4 1 -.8; .2 -.8 1];
Z = mvnrnd([0 0 0], Rho, n);
U = normcdf(Z,0,1);
X = [gaminv(U(:,1),2,1) betainv(U(:,2),2,2) tinv(U(:,3),5)];
plot3(X(:,1),X(:,2),X(:,3),<span class="string">'.'</span>);
grid <span class="string">on</span>; view([-55, 15]);
xlabel(<span class="string">'U1'</span>); ylabel(<span class="string">'U2'</span>); zlabel(<span class="string">'U3'</span>);
</pre><img vspace="5" hspace="5" src="../copulademo_11.png" alt=""> <p>線形相関パラメーター rho と、たとえば、ケンドールの tau の相関は、ここで使用される相関行列 Rho の各要素に対して成り立つことに注意してください。データの標本化順位相関は、理論的な値にほとんど等しいことを確かめることができます。</p><pre class="codeinput">tauTheoretical = 2.*asin(Rho)./pi
</pre><pre class="codeoutput">
tauTheoretical =

    1.0000    0.2620    0.1282
    0.2620    1.0000   -0.5903
    0.1282   -0.5903    1.0000

</pre><pre class="codeinput">tauSample = corr(X, <span class="string">'type'</span>,<span class="string">'Kendall'</span>)
</pre><pre class="codeoutput">
tauSample =

    1.0000    0.2655    0.1060
    0.2655    1.0000   -0.6076
    0.1060   -0.6076    1.0000

</pre><h2>コピュラと経験的な周辺分布<a name="23"></a></h2><p>コピュラを使って従属多変量データをシミュレートするには、以下の項目を指定する必要があることを確認しました。</p><pre>  1) コピュラ族 (と任意の形状パラメーター)
  2) 変数間の順位相関
  3) 各変数に対する周辺分布</pre><p>株式に対して返された 2 つのデータセットがあり、データと同じ分布に従う入力を使ってモンテ・カルロ シミュレーションを実行するとします。</p><pre class="codeinput">load <span class="string">stockreturns</span>
nobs = size(stocks,1);
subplot(2,1,1); hist(stocks(:,1),10);
xlabel(<span class="string">'X1'</span>); ylabel(<span class="string">'Frequency'</span>);
subplot(2,1,2); hist(stocks(:,2),10);
xlabel(<span class="string">'X2'</span>); ylabel(<span class="string">'Frequency'</span>);
</pre><img vspace="5" hspace="5" src="../copulademo_12.png" alt=""> <p>これら 2 つのデータのベクトルは同じ長さですが、それは重要ではありません。</p><p>各データセットを別々にパラメトリック モデルで近似でき、それらの推定を周辺分布として使用できます。しかし、パラメトリック モデルは、十分に適応性がないことがあります。代わりに、経験的モデルを周辺分布として使用できます。累積分布逆関数を計算する方法のみが必要です。</p><p>これらのデータセットの経験的な累積分布逆関数は、1/nob、2/nobs、... 1 という値のステップをもつステップ関数です。1.  ステップの高さは並べられたデータです。</p><pre class="codeinput">invCDF1 = sort(stocks(:,1)); n1 = length(stocks(:,1));
invCDF2 = sort(stocks(:,2)); n2 = length(stocks(:,2));
subplot(1,1,1);
stairs((1:nobs)/nobs, invCDF1,<span class="string">'b'</span>);
hold <span class="string">on</span>; stairs((1:nobs)/nobs, invCDF2,<span class="string">'r'</span>); hold <span class="string">off</span>
legend(<span class="string">'X1'</span>,<span class="string">'X2'</span>);
xlabel(<span class="string">'Cumulative Probability'</span>); ylabel(<span class="string">'X'</span>);
</pre><img vspace="5" hspace="5" src="../copulademo_13.png" alt=""> <p>別のコピュラと相関を試み、シミュレートするとします。ここで、かなり大きな負の相関関係パラメーターをもつ 2 変量 t(2) コピュラを使用します。</p><pre class="codeinput">n = 1000;
rho = -.8;
nu = 5;
T = mvtrnd([1 rho; rho 1], nu, n);
U = tcdf(T,nu);
X = [invCDF1(ceil(n1*U(:,1))) invCDF2(ceil(n2*U(:,2)))];

[n1,ctr1] = hist(X(:,1),10);
[n2,ctr2] = hist(X(:,2),10);
subplot(2,2,2); plot(X(:,1),X(:,2),<span class="string">'.'</span>); axis([-3.5 3.5 -3.5 3.5]); h1 = gca;
title(<span class="string">'1000 Simulated Dependent Values'</span>);
xlabel(<span class="string">'X1'</span>); ylabel(<span class="string">'X2'</span>);
subplot(2,2,4); bar(ctr1,-n1,1); axis([-3.5 3.5 -max(n1)*1.1 0]); axis(<span class="string">'off'</span>); h2 = gca;
subplot(2,2,1); barh(ctr2,-n2,1); axis([-max(n2)*1.1 0 -3.5 3.5]); axis(<span class="string">'off'</span>); h3 = gca;
set(h1,<span class="string">'Position'</span>,[0.35 0.35 0.55 0.55]);
set(h2,<span class="string">'Position'</span>,[.35 .1 .55 .15]);
set(h3,<span class="string">'Position'</span>,[.1 .35 .15 .55]);
colormap([.8 .8 1]);
</pre><img vspace="5" hspace="5" src="../copulademo_14.png" alt=""> <p>シミュレートされたデータの周辺ヒストグラムは、オリジナル データの周辺ヒストグラムにほぼ一致し、もっと多くの値の組をシミュレートすると同じになります。値はオリジナル データから取得され、各データセットに観測値が 100 個しかないため、シミュレートされたデータは多少離散的です。これを解決する 1 つの方法は、できる限り正規分布に従った少量のランダムな変動を、シミュレートされた最終値に追加することです。これは、経験的な累積分布逆関数を平滑化したものを使用するのと同等です。</p><p class="footer">Copyright 2004-2009 The MathWorks, Inc.<br>Published with MATLAB&reg; 7.13</p><p class="footer" id="trademarks">MATLAB and Simulink are registered trademarks of The MathWorks, Inc.  Please see <a href="http://www.mathworks.com/trademarks">www.mathworks.com/trademarks</a> for a list of other trademarks owned by The MathWorks, Inc.  Other product or brand names are trademarks or registered trademarks of their respective owners.</p></div><!-- ##### SOURCE BEGIN ##### %% Simulating Dependent Random Variables Using Copulas % This example shows you how to use copulas to generate data from multivariate % distributions when there are complicated relationships among the variables, or  % when the individual variables are from different distributions. % % MATLAB(R) is an ideal tool for running simulations that incorporate random % inputs or noise.  The Statistics Toolbox(TM) provides functions to create % sequences of random data according to many common univariate % distributions.  The Toolbox also includes a few functions to generate % random data from multivariate distributions, such as the multivariate % normal and multivariate t.  However, there is no built-in way to generate % multivariate distributions for all marginal distribtions, or in cases % where the individual variables are from different distributions. % % Recently, copulas have become popular in simulation models.  Copulas are % functions that describe dependencies among variables, and provide a way % to create distributions to model correlated multivariate data.  Using a % copula, a data analyst can construct a multivariate distribution by % specifying marginal univariate distributions, and choosing a particular % copula to provide a correlation structure between variables.  Bivariate % distributions, as well as distributions in higher dimensions, are % possible.  In this example, we discuss how to use copulas to generate % dependent multivariate random data in MATLAB, using the Statistics Toolbox.  %   Copyright 2004-2011 The MathWorks, Inc. %   $Revision: 1.1.6.6 $  $Date: 2012/02/14 03:55:32 $   %% Dependence Between Simulation Inputs % One of the design decisions for a Monte-Carlo simulation is a choice of % probability distributions for the random inputs.  Selecting a % distribution for each individual variable is often straightforward, but % deciding what dependencies should exist between the inputs may not be. % Ideally, input data to a simulation should reflect what is known about % dependence among the real quantities being modelled.  However, there % may be little or no information on which to base any dependence in the % simulation, and in such cases, it is a good idea to experiment with % different possibilities, in order to determine the model's sensitivity. % % However, it can be difficult to actually generate random inputs with % dependence when they have distributions that are not from a standard % multivariate distribution.  Further, some of the standard multivariate % distributions can model only very limited types of dependence.  It's % always possible to make the inputs independent, and while that is a % simple choice, it's not always sensible and can lead to the wrong % conclusions. % % For example, a Monte-Carlo simulation of financial risk might have random % inputs that represent different sources of insurance losses.  These % inputs might be modeled as lognormal random variables.  A reasonable % question to ask is how dependence between these two inputs affects the % results of the simulation.  Indeed, it might be known from real data that % the same random conditions affect both sources, and ignoring that in % the simulation could lead to the wrong conclusions. % % Simulation of independent lognormal random variables is trivial.  The % simplest way would be to use the |lognrnd| function.  Here, we'll use the % |mvnrnd| function to generate n pairs of independent normal random % variables, and then exponentiate them.  Notice that the covariance matrix % used here is diagonal, i.e., independence between the columns of Z. n = 1000; sigma = .5; SigmaInd = sigma.^2 .* [1 0; 0 1] %% ZInd = mvnrnd([0 0], SigmaInd, n); XInd = exp(ZInd); plot(XInd(:,1),XInd(:,2),'.'); axis equal; axis([0 5 0 5]); xlabel('X1'); ylabel('X2'); %% % Dependent bivariate lognormal r.v.'s are also easy to generate, using a % covariance matrix with non-zero off-diagonal terms. rho = .7; SigmaDep = sigma.^2 .* [1 rho; rho 1] %% ZDep = mvnrnd([0 0], SigmaDep, n); XDep = exp(ZDep); %% % A second scatter plot illustrates the difference between these two bivariate % distributions. plot(XDep(:,1),XDep(:,2),'.'); axis equal; axis([0 5 0 5]); xlabel('X1'); ylabel('X2');  %% % It's clear that there is more of a tendency in the second dataset for % large values of X1 to be associated with large values of X2, and % similarly for small values.  This dependence is determined by the % correlation parameter, rho, of the underlying bivariate normal.  The % conclusions drawn from the simulation could well depend on whether or not % X1 and X2 were generated with dependence or not.  %% % The bivariate lognormal distribution is a simple solution in the case, % and of course easily generalizes to higher dimensions and cases where the % marginal distributions are _different_ lognormals.  Other multivariate % distributions also exist, for example, the multivariate t and the % Dirichlet distributions are used to simulate dependent t and beta random % variables, respectively.  But the list of simple multivariate % distributions is not long, and they only apply in cases where the % marginals are all in the same family (or even the exact same % distributions).  This can be a real limitation in many situations.   %% A More General Method for Constructing Dependent Bivariate Distributions % Although the above construction that creates a bivariate lognormal is % simple, it serves to illustrate a method which is more generally % applicable.  First, we generate pairs of values from a bivariate normal % distribution.  There is statistical dependence between these two % variables, and each has a normal marginal distribution. Next, a % transformation (the exponential function) is applied separately to each % variable, changing the marginal distributions into lognormals. The % transformed variables still have a statistical dependence. % % If a suitable transformation could be found, this method could be % generalized to create dependent bivariate random vectors with other % marginal distributions.  In fact, a general method of constructing such a % transformation does exist, although not as simple as just exponentiation. % % By definition, applying the normal CDF (denoted here by PHI) to a standard % normal random variable results in a r.v. that is uniform on the % interval [0, 1].  To see this, if Z has a standard normal distribution, % then the CDF of U = PHI(Z) is % %    Pr{U <= u0} = Pr{PHI(Z) <= u0} = Pr{Z <= PHI^(-1)(u0)} = u0, % % and that is the CDF of a U(0,1) r.v.  Histograms of some simulated normal % and transformed values demonstrate that fact. n = 1000; z = normrnd(0,1,n,1); hist(z,-3.75:.5:3.75); xlim([-4 4]); title('1000 Simulated N(0,1) Random Values'); xlabel('Z'); ylabel('Frequency'); %% u = normcdf(z); hist(u,.05:.1:.95); title('1000 Simulated N(0,1) Values Transformed to U(0,1)'); xlabel('U'); ylabel('Frequency');  %% % Now, borrowing from the theory of univariate random number generation, % applying the inverse CDF of any distribution F to a U(0,1) random % variable results in a r.v. whose distribution is exactly F.  This is known % as the Inversion Method.  The proof is essentially the opposite of the % above proof for the forward case.  Another histogram illustrates the % transformation to a gamma distribution. x = gaminv(u,2,1); hist(x,.25:.5:9.75); title('1000 Simulated N(0,1) Values Transformed to Gamma(2,1)'); xlabel('X'); ylabel('Frequency');  %% % This two-step transformation can be applied to each variable of a % standard bivariate normal, creating dependent r.v.'s with arbitrary % marginal distributions.  Because the transformation works on each % component separately, the two resulting r.v.'s need not even have the % same marginal distributions.  The transformation is defined as % %    Z = [Z1 Z2] ~ N([0 0],[1 rho; rho 1]) %    U = [PHI(Z1) PHI(Z2)] %    X = [G1(U1) G2(U2)] % % where G1 and G2 are inverse CDFs of two possibly different distributions. % For example, we can generate random vectors from a bivariate distribution % with t(5) and Gamma(2,1) marginals. n = 1000; rho = .7; Z = mvnrnd([0 0], [1 rho; rho 1], n); U = normcdf(Z); X = [gaminv(U(:,1),2,1) tinv(U(:,2),5)];  %% % This plot has histograms alongside a scatter plot to show both the % marginal distributions, and the dependence. [n1,ctr1] = hist(X(:,1),20); [n2,ctr2] = hist(X(:,2),20); subplot(2,2,2); plot(X(:,1),X(:,2),'.'); axis([0 12 -8 8]); h1 = gca; title('1000 Simulated Dependent t and Gamma Values'); xlabel('X1 ~ Gamma(2,1)'); ylabel('X2 ~ t(5)'); subplot(2,2,4); bar(ctr1,-n1,1); axis([0 12 -max(n1)*1.1 0]); axis('off'); h2 = gca; subplot(2,2,1); barh(ctr2,-n2,1); axis([-max(n2)*1.1 0 -8 8]); axis('off'); h3 = gca; set(h1,'Position',[0.35 0.35 0.55 0.55]); set(h2,'Position',[.35 .1 .55 .15]); set(h3,'Position',[.1 .35 .15 .55]); colormap([.8 .8 1]);   %% Rank Correlation Coefficients % Dependence between X1 and X2 in this construction is determined by the % correlation parameter, rho, of the underlying bivariate normal. However, % it is _not_ true that the linear correlation of X1 and X2 is rho.  For % example, in the original lognormal case, there is a closed form for that % correlation: % %    cor(X1,X2) = (exp(rho.*sigma.^2) - 1) ./ (exp(sigma.^2) - 1) % % which is strictly less than rho unless rho is exactly one.  In more % general cases, though, such as the Gamma/t construction above, the linear % correlation between X1 and X2 is difficult or impossible to express in % terms of rho, but simulations can be used to show that the same effect % happens. % % That's because the linear correlation coefficient expresses the _linear_ % dependence between r.v.'s, and when nonlinear transformations are applied % to those r.v.'s, linear correlation is not preserved.  Instead, a rank % correlation coefficient, such as Kendall's tau or Spearman's rho, is more % appropriate. % % Roughly speaking, these rank correlations measure the degree to which % large or small values of one r.v. associate with large or small values of % another. However, unlike the linear correlation coefficient, they % measure the association only in terms of ranks.  As a consequence, the % rank correlation is preserved under any monotonic transformation.  In % particular, the transformation method just described preserves the rank % correlation.  Therefore, knowing the rank correlation of the bivariate % normal Z exactly determines the rank correlation of the final transformed % r.v.'s X.  While rho is still needed to parameterize the underlying % bivariate normal, Kendall's tau or Spearman's rho are more useful in % describing the dependence between r.v.'s, because they are invariant to the % choice of marginal distribution. % % It turns out that for the bivariate normal, there is a simple 1-1 mapping % between Kendall's tau or Spearman's rho, and the linear correlation % coefficient rho: % %    tau   = (2/pi)*arcsin(rho)     or   rho = sin(tau*pi/2) %    rho_s = (6/pi)*arcsin(rho/2)   or   rho = 2*sin(rho_s*pi/6) subplot(1,1,1); rho = -1:.01:1; tau = 2.*asin(rho)./pi; rho_s = 6.*asin(rho./2)./pi; plot(rho,tau,'-', rho,rho_s,'-', [-1 1],[-1 1],'k:'); axis([-1 1 -1 1]); xlabel('rho'); ylabel('Rank correlation coefficient'); legend('Kendall''s tau', 'Spearman''s rho_s', 'location','northwest');  %% % Thus, it's easy to create the desired rank correlation between X1 and X2, % regardless of their marginal distributions, by choosing the correct rho % parameter value for the linear correlation between Z1 and Z2. % % Notice that for the multivariate normal distribution, Spearman's rank % correlation is almost identical to the linear correlation.  However, this % is not true once we transform to the final random variables.   %% Copulas % The first step of the construction described above defines what is known % as a copula, specifically, a Gaussian copula.  A bivariate copula is % simply a probability distribution on two random variables, each of whose % marginal distributions is uniform.  These two variables may be completely % independent, deterministically related (e.g., U2 = U1), or anything in % between.  The family of bivariate Gaussian copulas is parameterized by % Rho = [1 rho; rho 1], the linear correlation matrix.  U1 and U2 approach % linear dependence as rho approaches +/- 1, and approach complete % independence as rho approaches zero. % % Scatter plots of some simulated random values for various levels of rho % illustrate the range of different possibilities for Gaussian copulas: n = 500; Z = mvnrnd([0 0], [1 .8; .8 1], n); U = normcdf(Z,0,1); subplot(2,2,1); plot(U(:,1),U(:,2),'.'); title('rho = 0.8'); xlabel('U1'); ylabel('U2'); Z = mvnrnd([0 0], [1 .1; .1 1], n); U = normcdf(Z,0,1); subplot(2,2,2); plot(U(:,1),U(:,2),'.'); title('rho = 0.1'); xlabel('U1'); ylabel('U2'); Z = mvnrnd([0 0], [1 -.1; -.1 1], n); U = normcdf(Z,0,1); subplot(2,2,3); plot(U(:,1),U(:,2),'.'); title('rho = -0.1'); xlabel('U1'); ylabel('U2'); Z = mvnrnd([0 0], [1 -.8; -.8 1], n); U = normcdf(Z,0,1); subplot(2,2,4); plot(U(:,1),U(:,2),'.'); title('rho = -0.8'); xlabel('U1'); ylabel('U2');  %% % The dependence between U1 and U2 is completely separate from the marginal % distributions of X1 = G(U1) and X2 = G(U2).  X1 and X2 can be given _any_ % marginal distributions, and still have the same rank correlation. % This is one of the main appeals of copulas REPLACE_WITH_DASH_DASH they allow this separate % specification of dependence and marginal distribution.   %% t Copulas % A different family of copulas can be constructed by starting from a % bivariate t distribution, and transforming using the corresponding t CDF. % The bivariate t distribution is parameterized with Rho, the linear % correlation matrix, and nu, the degrees of freedom.  Thus, for example, % we can speak of a t(1) or a t(5) copula, based on the multivariate t with % one and five degrees of freedom, respectively. % % Scatter plots of some simulated random values for various levels of rho % illustrate the range of different possibilities for t(1) copulas: n = 500; nu = 1; T = mvtrnd([1 .8; .8 1], nu, n); U = tcdf(T,nu); subplot(2,2,1); plot(U(:,1),U(:,2),'.'); title('rho = 0.8'); xlabel('U1'); ylabel('U2'); T = mvtrnd([1 .1; .1 1], nu, n); U = tcdf(T,nu); subplot(2,2,2); plot(U(:,1),U(:,2),'.'); title('rho = 0.1'); xlabel('U1'); ylabel('U2'); T = mvtrnd([1 -.1; -.1 1], nu, n); U = tcdf(T,nu); subplot(2,2,3); plot(U(:,1),U(:,2),'.'); title('rho = -0.1'); xlabel('U1'); ylabel('U2'); T = mvtrnd([1 -.8; -.8 1], nu, n); U = tcdf(T,nu); subplot(2,2,4); plot(U(:,1),U(:,2),'.'); title('rho = -0.8'); xlabel('U1'); ylabel('U2');  %% % A t copula has uniform marginal distributions for U1 and U2, just as a % Gaussian copula does.  The rank correlation tau or rho_s between % components in a t copula is also the same function of rho as for a % Gaussian.  However, as these plots demonstrate, a t(1) copula differs % quite a bit from a Gaussian copula, even when their components have the % same rank correlation.  The difference is in their dependence structure. % Not surprisingly, as the degrees of freedom parameter nu is made larger, % a t(nu) copula approaches the corresponding Gaussian copula. % % As with a Gaussian copula, any marginal distributions can be imposed over % a t copula.  For example, using a t copula with 1 degree of freedom, we % can again generate random vectors from a bivariate distribution with % Gam(2,1) and t(5) marginals: subplot(1,1,1); n = 1000; rho = .7; nu = 1; T = mvtrnd([1 rho; rho 1], nu, n); U = tcdf(T,nu); X = [gaminv(U(:,1),2,1) tinv(U(:,2),5)];  [n1,ctr1] = hist(X(:,1),20); [n2,ctr2] = hist(X(:,2),20); subplot(2,2,2); plot(X(:,1),X(:,2),'.'); axis([0 15 -10 10]); h1 = gca; title('1000 Simulated Dependent t and Gamma Values'); xlabel('X1 ~ Gamma(2,1)'); ylabel('X2 ~ t(5)'); subplot(2,2,4); bar(ctr1,-n1,1); axis([0 15 -max(n1)*1.1 0]); axis('off'); h2 = gca; subplot(2,2,1); barh(ctr2,-n2,1); axis([-max(n2)*1.1 0 -10 10]); axis('off'); h3 = gca; set(h1,'Position',[0.35 0.35 0.55 0.55]); set(h2,'Position',[.35 .1 .55 .15]); set(h3,'Position',[.1 .35 .15 .55]); colormap([.8 .8 1]);  %% % Compared to the bivariate Gamma/t distribution constructed earlier, which % was based on a Gaussian copula, the distribution constructed here, based % on a t(1) copula, has the same marginal distributions and the same rank % correlation between variables, but a very different dependence structure. % This illustrates the fact that multivariate distributions are not % uniquely defined by their marginal distributions, or by their % correlations.  The choice of a particular copula in an application may be % based on actual observed data, or different copulas may be used as a way % of determining the sensitivity of simulation results to the input % distribution.   %% Higher-Order Copulas % The Gaussian and t copulas are known as elliptical copulas.  It's easy to % generalize elliptical copulas to a higher number of dimensions.  For % example, we can simulate data from a trivariate distribution with Gamma(2,1), % Beta(2,2), and t(5) marginals using a Gaussian copula as follows. subplot(1,1,1); n = 1000; Rho = [1 .4 .2; .4 1 -.8; .2 -.8 1]; Z = mvnrnd([0 0 0], Rho, n); U = normcdf(Z,0,1); X = [gaminv(U(:,1),2,1) betainv(U(:,2),2,2) tinv(U(:,3),5)]; plot3(X(:,1),X(:,2),X(:,3),'.'); grid on; view([-55, 15]); xlabel('U1'); ylabel('U2'); zlabel('U3');  %% % Notice that the relationship between the linear correlation parameter rho % and, for example, Kendall's tau, holds for each entry in the correlation % matrix Rho used here.  We can verify that the sample rank correlations of % the data are approximately equal to the theoretical values. tauTheoretical = 2.*asin(Rho)./pi %% tauSample = corr(X, 'type','Kendall')   %% Copulas and Empirical Marginal Distributions % To simulate dependent multivariate data using a copula, we have seen that % we need to specify % %    1) the copula family (and any shape parameters), %    2) the rank correlations among variables, and %    3) the marginal distributions for each variable % % Suppose we have two sets of stock return data, and we would like to run a % Monte Carlo simulation with inputs that follow the same distributions as % our data. load stockreturns nobs = size(stocks,1); subplot(2,1,1); hist(stocks(:,1),10); xlabel('X1'); ylabel('Frequency'); subplot(2,1,2); hist(stocks(:,2),10); xlabel('X2'); ylabel('Frequency');  %% % (These two data vectors have the same length, but that is not crucial.) % % We could fit a parametric model separately to each dataset, and use those % estimates as our marginal distributions.  However, a parametric model may % not be sufficiently flexible.  Instead, we can use an empirical model % for the marginal distributions.  We only need a way to compute the % inverse CDF. % % The empirical inverse CDF for these datasets is just a stair function, % with steps at the values 1/nobs, 2/nobs, ... 1.  The step heights are % simply the sorted data. invCDF1 = sort(stocks(:,1)); n1 = length(stocks(:,1)); invCDF2 = sort(stocks(:,2)); n2 = length(stocks(:,2)); subplot(1,1,1); stairs((1:nobs)/nobs, invCDF1,'b'); hold on; stairs((1:nobs)/nobs, invCDF2,'r'); hold off legend('X1','X2'); xlabel('Cumulative Probability'); ylabel('X');  %% % For the simulation, we might want to experiment with different copulas % and correlations.  Here, we'll use a bivariate t(2) copula with a fairly % large negative correlation parameter. n = 1000; rho = -.8; nu = 5; T = mvtrnd([1 rho; rho 1], nu, n); U = tcdf(T,nu); X = [invCDF1(ceil(n1*U(:,1))) invCDF2(ceil(n2*U(:,2)))];  [n1,ctr1] = hist(X(:,1),10); [n2,ctr2] = hist(X(:,2),10); subplot(2,2,2); plot(X(:,1),X(:,2),'.'); axis([-3.5 3.5 -3.5 3.5]); h1 = gca; title('1000 Simulated Dependent Values'); xlabel('X1'); ylabel('X2'); subplot(2,2,4); bar(ctr1,-n1,1); axis([-3.5 3.5 -max(n1)*1.1 0]); axis('off'); h2 = gca; subplot(2,2,1); barh(ctr2,-n2,1); axis([-max(n2)*1.1 0 -3.5 3.5]); axis('off'); h3 = gca; set(h1,'Position',[0.35 0.35 0.55 0.55]); set(h2,'Position',[.35 .1 .55 .15]); set(h3,'Position',[.1 .35 .15 .55]); colormap([.8 .8 1]);  %% % The marginal histograms of the simulated data closely match those of the % original data, and would become identical as we simulate more pairs of % values.  Notice that the values are drawn from the original data, and % because there are only 100 observations in each dataset, the simulated % data are somewhat "discrete".  One way to overcome this would be to % add a small amount of random variation, possibly normally distributed, to % the final simulated values.  This is equivalent to using a smoothed % version of the empirical inverse CDF.   displayEndOfDemoMessage(mfilename)  ##### SOURCE END ##### --></body></html>