
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- This HTML was auto-generated from MATLAB code. To make changes, update the MATLAB code and republish this document.       --><title>クラスター分析</title><meta name="generator" content="MATLAB 7.14"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2011-11-14"><meta name="DC.source" content="clusterdemo.m"><link rel="stylesheet" type="text/css" href="../../../../matlab/helptools/private/style.css"><link rel="stylesheet" type="text/css" href="../../../../matlab/helptools/private/style_ja_JP.css"></head><body><div class="header"><div class="left"><a href="matlab:edit clusterdemo">エディターで clusterdemo.m を開く</a></div><div class="right"><a href="matlab:echodemo clusterdemo">コマンド ウィンドウで実行</a></div></div><div class="content"><h1>クラスター分析</h1><!--introduction--><p>クラスター分析は、観測つまりオブジェクトの類似度と非類似度を検討する方法です。データはしばしば観測のグループ、つまりクラスターに自然に分類されます。そこでは、同じクラスター内のオブジェクトの特性は類似しており、別のクラスター内のオブジェクトの特性は異なります。</p><p>これは、Statistics Toolbox™ を使ってクラスター分析をどのように実行するかを示すデモです。</p><!--/introduction--><h2>目次</h2><div><ul><li><a href="#1">K 平均クラスタリングと階層クラスタリング</a></li><li><a href="#3">フィッシャーのアヤメのデータ</a></li><li><a href="#4">K 平均クラスタリングを使用するフィッシャーのアヤメのデータのクラスタリング</a></li><li><a href="#17">階層クラスタリングを使用するフィッシャーのアヤメのデータのクラスタリング</a></li></ul></div><h2>K 平均クラスタリングと階層クラスタリング<a name="1"></a></h2><p>Statistics Toolbox には、K 平均クラスタリングと階層クラスタリングという 2 種類のクラスター分析を実行するための関数があります。</p><p>K 平均クラスタリングとは、データ内の観測を位置と互いからの距離をもつオブジェクトとして扱う分割法の一種です。K 平均クラスタリングでは、オブジェクトを k 個の互いに排他的なクラスターに分割します。同じクラスター内のオブジェクトはできるだけ近くに、別のクラスター内のオブジェクトからはできるだけ遠くにあるように分割します。各クラスターは重心、つまり中心となる点で特徴付けられます。もちろん、クラスタリングで使用される距離が空間的距離を表すことはまれです。</p><p>階層クラスタリングは、クラスター木を作成することにより、さまざまな距離のスケールで同時にデータのグループ化を調べる方法です。この木は、K 平均とは異なり、複数のクラスターで構成される 1 つの集合ではなく、ある 1 つのレベルに存在する複数のクラスターがまとまって次の 1 段高いレベルでクラスターとなる複数レベルの階層です。これにより、クラスタリングのどのレベルまたはスケールがアプリケーションにおいて最適であるかを決定することが可能になります。</p><p>例で使用される一部の関数は、MATLAB&reg; の組み込み乱数生成関数を呼び出します。このデモで示された結果を再現するには、次のコマンドを実行して乱数発生器を既知状態に設定する必要があります。状態を設定しないと、些細な点で結果が異なる可能性があります。たとえば、クラスター番号が変わってしまうかもしれません。また、最適とは言えないクラスター解を得ることになる可能性もあります (デモでは最適とは言えない解やそれらを回避する方法などについての議論が行われます)。</p><pre class="codeinput">rng(14,<span class="string">'twister'</span>);
</pre><h2>フィッシャーのアヤメのデータ<a name="3"></a></h2><p>1920 年代に植物学者たちは、アヤメの標本 150 個 (3 種について 50 個ずつ) のがく片の長さと幅、花弁の長さと幅に関する測定値を収集しました。この測定値は、「フィッシャーのアヤメのデータ」として知られるようになりました。</p><p>このデータの各観測は、1 つの既知種に由来するので、データをグループ化するための明らかな方法が既に存在することになります。差し当たり、種情報は無視して、生の測定値だけを使用してデータをクラスター化することにします。完了したら、結果として得られたクラスターと実際の種を比較し、3 種類のアヤメに明確に識別できる特徴があるかどうかを調べます。</p><h2>K 平均クラスタリングを使用するフィッシャーのアヤメのデータのクラスタリング<a name="4"></a></h2><p>関数 <tt>kmeans</tt> は K 平均クラスタリングを実行します。その際、オブジェクトをクラスターに割り当てる反復アルゴリズムを使用して、すべてのクラスターについて各オブジェクトからそのクラスターの重心までの距離の総和を最小にします。この関数をフィッシャーのアヤメのデータに対して実行すると、アヤメの標本ががく片と花弁の測定値に基づいていくつかのグループに自然に分けられます。K 平均クラスタリングでは、作成するクラスターの数を指定しなければなりません。</p><p>最初に、データを読み込み、関数 <tt>kmeans</tt> を呼び出します。その際、目的のクラスター数を 2 に設定し、ユークリッド距離の 2 乗を使用します。結果のクラスターがいかにうまく分離されるかについて知るには、シルエット プロットを作成します。シルエット・プロットは、1個のクラスター中の各点が近隣のクラスター中の点にどれくらい接近してあるかの基準を表示します。</p><pre class="codeinput">load <span class="string">fisheriris</span>
[cidx2,cmeans2] = kmeans(meas,2,<span class="string">'dist'</span>,<span class="string">'sqeuclidean'</span>);
[silh2,h] = silhouette(meas,cidx2,<span class="string">'sqeuclidean'</span>);
</pre><img vspace="5" hspace="5" src="../clusterdemo_01.png" alt=""> <p>シルエット プロットから、両方のクラスターの大部分の点が 0.8 よりも大きい大きなシルエットの値をもつことがわかります。これは、それらの点が近隣するクラスターから十分に離れていることを示します。ただし、各クラスターにはシルエット値が低い点もいくつか含まれています。これは、それらの点が他のクラスターの点に近いことを示します。</p><p>このデータの 4 番目の測定値である花弁の幅は、3 番目の測定値である花弁の長さと高い相関関係を示しています。したがって、最初の 3 つの測定値の 3 次元プロットは、次元数を 4 にしなくてもデータをうまく表現することになります。関数 <tt>kmeans</tt> によって作成されるクラスターごとに別の記号を使用してデータをプロットする場合には、シルエット値が小さい点は他のクラスターの点に近い点であるとわかります。</p><pre class="codeinput">ptsymb = {<span class="string">'bs'</span>,<span class="string">'r^'</span>,<span class="string">'md'</span>,<span class="string">'go'</span>,<span class="string">'c+'</span>};
<span class="keyword">for</span> i = 1:2
    clust = find(cidx2==i);
    plot3(meas(clust,1),meas(clust,2),meas(clust,3),ptsymb{i});
    hold <span class="string">on</span>
<span class="keyword">end</span>
plot3(cmeans2(:,1),cmeans2(:,2),cmeans2(:,3),<span class="string">'ko'</span>);
plot3(cmeans2(:,1),cmeans2(:,2),cmeans2(:,3),<span class="string">'kx'</span>);
hold <span class="string">off</span>
xlabel(<span class="string">'Sepal Length'</span>); ylabel(<span class="string">'Sepal Width'</span>); zlabel(<span class="string">'Petal Length'</span>);
view(-137,10);
grid <span class="string">on</span>
</pre><img vspace="5" hspace="5" src="../clusterdemo_02.png" alt=""> <p>各クラスターの重心は、円で囲んだ X でプロットされます。低いクラスターの 3 点は、三角形でプロットされ、正方形でプロットされる上のクラスターの点に非常に近い点です。しかし実際には、上のクラスターは広い範囲に広がっているので、これらの 3 点は上のクラスターの重心よりむしろ低いクラスターの重心に近い位置に存在することになります。これは、ギャップによりクラスター内での点の塊から離れている場合でも当てはまります。K 平均クラスタリングでは、距離だけを考慮し密度は考慮されないので、このような結果が生じることがあります。</p><p>クラスター数を増やして、関数 <tt>kmeans</tt> がデータ内でグループ化構造をさらに見つけることができるかどうかを調べることができます。今回は、クラスタリング アルゴリズムでオプションの 'display' パラメーターを使用して、各反復に関する情報を出力します。</p><pre class="codeinput">[cidx3,cmeans3] = kmeans(meas,3,<span class="string">'display'</span>,<span class="string">'iter'</span>);
</pre><pre class="codeoutput">  iter	 phase	     num	         sum
     1	     1	     150	     86.2619
     2	     1	       7	     79.5415
     3	     1	       3	     78.8514
     4	     2	       0	     78.8514
4 iterations, total sum of distances = 78.8514.
</pre><p>反復ごとに、<tt>kmeans</tt> アルゴリズムは点をクラスター間で再度割り当てて、点と重心間距離の総和を減らしてから、新しいクラスター割り当てのクラスター重心を計算し直します。距離の総和と再割り当て回数は、アルゴリズムが最小値に到達するまで、反復ごとに減少していくことに注意してください。関数 <tt>kmeans</tt> で使用されるアルゴリズムは、2 段階で構成されます。前の例では、アルゴリズムの第 2 段階で再割り当てがまったく行われませんでした。これは、2、3 回ほどの反復後に第 1 段階で最小値に達したことを示しています。</p><p>既定の設定では、関数 <tt>kmeans</tt> は無作為に選択された初期の重心位置の集合を使用して処理を開始します。数値的な最小化の他の多くの場合のように、関数 <tt>kmeans</tt> が到達することがある解は、出発点に依存します。そして、この関数が局所的最小値に到達することは可能です。この場合、任意の 1 点を新しいクラスターに再び割り当てると、点と重心間距離の総和が増加しますが、より適切な解は確かに存在します。しかし、その問題を解決するためには、オプションのパラメーター 'replicates' を使用することができます。複数回反復するよう指定すると、関数 <tt>kmeans</tt> は 1 回ごとに無作為に選択されたさまざまな重心からクラスタリング処理を繰り返します。</p><pre class="codeinput">[cidx3,cmeans3,sumd3] = kmeans(meas,3,<span class="string">'replicates'</span>,5,<span class="string">'display'</span>,<span class="string">'final'</span>);
</pre><pre class="codeoutput">6 iterations, total sum of distances = 78.8514.
8 iterations, total sum of distances = 78.8514.
3 iterations, total sum of distances = 78.8514.
11 iterations, total sum of distances = 142.754.
7 iterations, total sum of distances = 78.8514.
</pre><p>出力は、この比較的簡単な問題に対してさえ、大域的でない最小が存在することを示します。この 5 回の繰り返しのそれぞれは、別の初期重心の集合から開始されました。出発点に応じて、関数 <tt>kmeans</tt> は 2 つの異なる解のどちらかに到達しました。しかし、<tt>kmeans</tt> が返す最終的な解は、繰り返しの全体にわたって、距離の総和の最小をもちます。3 番目の出力引数には、その最適解について各クラスター内での距離の総和が格納されます。</p><pre class="codeinput">sum(sumd3)
</pre><pre class="codeoutput">
ans =

   78.8514

</pre><p>この 3 クラスター解のシルエット プロットは、十分に離れたクラスターが 1 つ存在するが、他の 2 つのクラスターはあまり離れていないことを示しています。</p><pre class="codeinput">[silh3,h] = silhouette(meas,cidx3,<span class="string">'sqeuclidean'</span>);
</pre><img vspace="5" hspace="5" src="../clusterdemo_03.png" alt=""> <p>ここで再度、生データをプロットして、関数 <tt>kmeans</tt> が点をクラスターに割り当てる様子を確認します。</p><pre class="codeinput"><span class="keyword">for</span> i = 1:3
    clust = find(cidx3==i);
    plot3(meas(clust,1),meas(clust,2),meas(clust,3),ptsymb{i});
    hold <span class="string">on</span>
<span class="keyword">end</span>
plot3(cmeans3(:,1),cmeans3(:,2),cmeans3(:,3),<span class="string">'ko'</span>);
plot3(cmeans3(:,1),cmeans3(:,2),cmeans3(:,3),<span class="string">'kx'</span>);
hold <span class="string">off</span>
xlabel(<span class="string">'Sepal Length'</span>); ylabel(<span class="string">'Sepal Width'</span>); zlabel(<span class="string">'Petal Length'</span>);
view(-137,10);
grid <span class="string">on</span>
</pre><img vspace="5" hspace="5" src="../clusterdemo_04.png" alt=""> <p>関数 <tt>kmeans</tt> が上のクラスターを 2 クラスター解から分離したこと、およびこの 2 つのクラスターが互いに非常に近い位置にあることがわかります。クラスタリング後にこのデータで何を行うのかに応じて、この 3 クラスター解が前の 2 クラスター解より便利なのかそれとも不便なのかが決まります。関数 <tt>silhouette</tt> の最初の出力引数には、各点のシルエット値が格納されます。これを使用して 2 つの解を定量的に比較することができます。2 クラスター解の平均シルエット値は大きかったのですが、これは、離れたクラスターを作成するという観点から純粋に見て、より優れた解であることを示しています。</p><pre class="codeinput">[mean(silh2) mean(silh3)]
</pre><pre class="codeoutput">
ans =

    0.8504    0.7357

</pre><p>これらのデータを別の距離でクラスター化することもできます。これらのデータでは、コサイン距離を使用するのが理にかなうかもしれません。測定値の絶対サイズは無視され、相対サイズだけが考慮されるからです。したがって、がく片と花弁のサイズは異なっていても形状が似ていた 2 本の花は、ユークリッド距離の 2 乗に関しては近くないかもしれませんが、コサイン距離に関しては近いと言えます。</p><pre class="codeinput">[cidxCos,cmeansCos] = kmeans(meas,3,<span class="string">'dist'</span>,<span class="string">'cos'</span>);
</pre><p>シルエット プロットから考えて、これらのクラスターは、ユークリッド距離の 2 乗を使用して見つかったクラスターよりもわずかに離れているように思われます。</p><pre class="codeinput">[silhCos,h] = silhouette(meas,cidxCos,<span class="string">'cos'</span>);
[mean(silh2) mean(silh3) mean(silhCos)]
</pre><pre class="codeoutput">
ans =

    0.8504    0.7357    0.7491

</pre><img vspace="5" hspace="5" src="../clusterdemo_05.png" alt=""> <p>クラスターの順序が前のシルエット プロットとは異なることに注意してください。これは、関数 <tt>kmeans</tt> が初期クラスター割り当てを無作為に選択するからです。</p><p>生データをプロットすることにより、2 つの異なる距離を使用して作成されたクラスター形状の違いを確認することができます。この 2 つの解は似ていますが、2 つの上のクラスターはコサイン距離を使用するときの原点の向きに延長されます。</p><pre class="codeinput"><span class="keyword">for</span> i = 1:3
    clust = find(cidxCos==i);
    plot3(meas(clust,1),meas(clust,2),meas(clust,3),ptsymb{i});
    hold <span class="string">on</span>
<span class="keyword">end</span>
hold <span class="string">off</span>
xlabel(<span class="string">'Sepal Length'</span>); ylabel(<span class="string">'Sepal Width'</span>); zlabel(<span class="string">'Petal Length'</span>);
view(-137,10);
grid <span class="string">on</span>
</pre><img vspace="5" hspace="5" src="../clusterdemo_06.png" alt=""> <p>このプロットにはクラスターの重心は含まれません。コサイン距離に関する重心は、生データ空間の原点から伸びる半直線に対応するからです。ただし、正規化されたデータ ポイントの平行座標プロットを作成して、クラスター重心間の違いを可視化することができます。</p><pre class="codeinput">lnsymb = {<span class="string">'b-'</span>,<span class="string">'r-'</span>,<span class="string">'m-'</span>};
names = {<span class="string">'SL'</span>,<span class="string">'SW'</span>,<span class="string">'PL'</span>,<span class="string">'PW'</span>};
meas0 = meas ./ repmat(sqrt(sum(meas.^2,2)),1,4);
ymin = min(min(meas0));
ymax = max(max(meas0));
<span class="keyword">for</span> i = 1:3
    subplot(1,3,i); plot(meas0(cidxCos==i,:)',lnsymb{i});
    hold <span class="string">on</span>; plot(cmeansCos(i,:)',<span class="string">'k-'</span>,<span class="string">'LineWidth'</span>,2); hold <span class="string">off</span>;
    title(sprintf(<span class="string">'Cluster %d'</span>,i));
    set(gca,<span class="string">'Xlim'</span>,[.9 4.1],<span class="string">'XTick'</span>,1:4,<span class="string">'XTickLabel'</span>,names,<span class="string">'YLim'</span>,[ymin ymax])
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="../clusterdemo_07.png" alt=""> <p>このプロットから、3 つのクラスターそれぞれの標本のがく片と花弁は、平均して相対サイズが明らかに異なります。最初のクラスターには、花弁より明らかに小さいがく片があります。2 番目の 2 つのクラスターのがく片と花弁は、サイズに関して部分的に重なり合いますが、3 番目のクラスターのがく片と花弁の方が、2 番目より大きく重なり合っています。また、2 番目と 3 番目のクラスターには互いに非常に似通った標本がいくつかあるということもわかります。</p><p>データ内の各観測の種を知っているので、関数 <tt>kmeans</tt> が発見したクラスターと実際の種を比較して、3 種に見分けがつくほど固有の物理的な特徴があるかどうかを確認することができます。実際のところ、次のプロットが示すとおり、コサイン距離を使用して作成されたクラスターは、種グループとわずかに 5 本の花しか違いません。この 5 つの点は星型でプロットされており、すべて上の 2 つのクラスターの境界付近にあります。</p><pre class="codeinput">subplot(1,1,1);
<span class="keyword">for</span> i = 1:3
    clust = find(cidxCos==i);
    plot3(meas(clust,1),meas(clust,2),meas(clust,3),ptsymb{i});
    hold <span class="string">on</span>
<span class="keyword">end</span>
xlabel(<span class="string">'Sepal Length'</span>); ylabel(<span class="string">'Sepal Width'</span>); zlabel(<span class="string">'Petal Length'</span>);
view(-137,10);
grid <span class="string">on</span>
sidx = grp2idx(species);
miss = find(cidxCos ~= sidx);
plot3(meas(miss,1),meas(miss,2),meas(miss,3),<span class="string">'k*'</span>);
legend({<span class="string">'setosa'</span>,<span class="string">'versicolor'</span>,<span class="string">'virginica'</span>});
hold <span class="string">off</span>
</pre><img vspace="5" hspace="5" src="../clusterdemo_08.png" alt=""> <h2>階層クラスタリングを使用するフィッシャーのアヤメのデータのクラスタリング<a name="17"></a></h2><p>K 平均クラスタリングでは、アヤメのデータの分割が 1 つしか生成されませんでしたが、データのグループ化のスケールをいろいろに変えて調べてみたいと思うかもしれません。階層クラスタリングを使用すると、複数のクラスターで構成される階層木を 1 つ作成することによりそうすることができます。</p><p>最初に、アヤメのデータでの観測間の距離を使用してクラスター木を作成します。ユークリッド距離を使用して開始します。</p><pre class="codeinput">eucD = pdist(meas,<span class="string">'euclidean'</span>);
clustTreeEuc = linkage(eucD,<span class="string">'average'</span>);
</pre><p>共表形相関は、クラスター木が元の距離に一致していることを確認するための方法の 1 つです。値が大きい場合、それは観測間の 2 つ 1 組の関連性が実際のペアワイズ距離と相関しているという意味で、木が距離をよく近似していることを示しています。この木は、距離をかなりよく近似しているようです。</p><pre class="codeinput">cophenet(clustTreeEuc,eucD)
</pre><pre class="codeoutput">
ans =

    0.8770

</pre><p>クラスターの階層を可視化するには、デンドログラムをプロットします。</p><pre class="codeinput">[h,nodes] = dendrogram(clustTreeEuc,0);
set(gca,<span class="string">'TickDir'</span>,<span class="string">'out'</span>,<span class="string">'TickLength'</span>,[.002 0],<span class="string">'XTickLabel'</span>,[]);
</pre><img vspace="5" hspace="5" src="../clusterdemo_09.png" alt=""> <p>この木のルート ノードは、残りのノードよりはるかに高い位置にあります。これは、K 平均クラスタリングでの結果を裏付けています。つまり、2 つの大きい異なる観測グループが存在するということです。この 2 つのグループそれぞれで、距離のスケールを下げるにつれて、低水準のグループが現れてくるのがわかります。レベル、サイズ、および相違度がさまざまに異なるグループがたくさんあります。</p><p>K 平均クラスタリングの結果に基づいて考えると、距離の測定法としてコサインも有望な選択肢かもしれません。結果として得られる階層木はまったく異なるものであり、これはアヤメのデータのグループ構造を見るまったく別の方法があることを示唆しています。</p><pre class="codeinput">cosD = pdist(meas,<span class="string">'cosine'</span>);
clustTreeCos = linkage(cosD,<span class="string">'average'</span>);
cophenet(clustTreeCos,cosD)
</pre><pre class="codeoutput">
ans =

    0.9360

</pre><pre class="codeinput">[h,nodes] = dendrogram(clustTreeCos,0);
set(gca,<span class="string">'TickDir'</span>,<span class="string">'out'</span>,<span class="string">'TickLength'</span>,[.002 0],<span class="string">'XTickLabel'</span>,[]);
</pre><img vspace="5" hspace="5" src="../clusterdemo_10.png" alt=""> <p>この木の最高水準により、アヤメの標本が 2 つの非常に異なるグループに分けられます。この図には、コサイン距離に関して、グループ間の違いと比較してグループ内の違いがユークリッド距離の場合よりはるかに小さいことが示されています。これこそまさに、このデータでは当然の結果と言えます。コサイン距離では原点から同じ &quot;向き&quot; にあるオブジェクトのゼロ対距離を計算するからです。</p><p>150 個の観測では、プロットが乱れますが、木の最低水準を表示しない単純化されたデンドログラムを作成することができます。</p><pre class="codeinput">[h,nodes] = dendrogram(clustTreeCos,12);
</pre><img vspace="5" hspace="5" src="../clusterdemo_11.png" alt=""> <p>木内にある 3 つの最高に高いノードによって、3 つのサイズが等しいグループおよび他のいずれにも近くない 1 つの標本 (ラベル表示はリーフ ノード 5) に分けられます。</p><pre class="codeinput">[sum(ismember(nodes,[11 12 9 10])) sum(ismember(nodes,[6 7 8])) <span class="keyword">...</span>
                  sum(ismember(nodes,[1 2 4 3])) sum(nodes==5)]
</pre><pre class="codeoutput">
ans =

    54    46    49     1

</pre><p>たいていの場合には、デンドログラムが十分な結果となります。もっとも、さらに 1 歩進んで関数 <tt>cluster</tt> を使用して、木を切り分け、K 平均の場合のように観測を特定のクラスターに明示的に分割することもできます。クラスターを作成するためのコサイン距離に基づく階層を使用しながら、3 つの最も高いノードの下で木を切る関連性の高さを指定し、クラスターを 4 つ作成し、クラスター化された生データをプロットします。</p><pre class="codeinput">hidx = cluster(clustTreeCos,<span class="string">'criterion'</span>,<span class="string">'distance'</span>,<span class="string">'cutoff'</span>,.006);
<span class="keyword">for</span> i = 1:5
    clust = find(hidx==i);
    plot3(meas(clust,1),meas(clust,2),meas(clust,3),ptsymb{i});
    hold <span class="string">on</span>
<span class="keyword">end</span>
hold <span class="string">off</span>
xlabel(<span class="string">'Sepal Length'</span>); ylabel(<span class="string">'Sepal Width'</span>); zlabel(<span class="string">'Petal Length'</span>);
view(-137,10);
grid <span class="string">on</span>
</pre><img vspace="5" hspace="5" src="../clusterdemo_12.png" alt=""> <p>このプロットは、コサイン距離を使用する階層クラスタリングの結果が、K 平均クラスタリングの結果と定性的に類似していることを 3 つのクラスターで示しています。ただし、階層的クラスター木を作成することにより、K 平均クラスタリングにおいて K の値をさまざまに変えて実験をかなり繰り返し実行しなければならないようにする要因を一度に可視化することができます。</p><p>階層クラスタリングでは、関連性をさまざまに変えて実験することもできます。たとえば、アヤメのデータを 1 つの関連性でクラスター化する場合、平均距離より離れているオブジェクトが連結されがちですが、データ構造のかなり異なる解釈を得ることができます。</p><pre class="codeinput">clustTreeSng = linkage(eucD,<span class="string">'single'</span>);
[h,nodes] = dendrogram(clustTreeSng,0);
set(gca,<span class="string">'TickDir'</span>,<span class="string">'out'</span>,<span class="string">'TickLength'</span>,[.002 0],<span class="string">'XTickLabel'</span>,[]);
</pre><img vspace="5" hspace="5" src="../clusterdemo_13.png" alt=""> <p class="footer">Copyright 2002-2011 The MathWorks, Inc.<br>Published with MATLAB&reg; 7.13</p><p class="footer" id="trademarks">MATLAB and Simulink are registered trademarks of The MathWorks, Inc.  Please see <a href="http://www.mathworks.com/trademarks">www.mathworks.com/trademarks</a> for a list of other trademarks owned by The MathWorks, Inc.  Other product or brand names are trademarks or registered trademarks of their respective owners.</p></div><!-- ##### SOURCE BEGIN ##### %% Cluster Analysis % Cluster analysis is a way to examine similarities and dissimilarities of % observations or objects. Data often fall naturally into groups, or % clusters, of observations, where the characteristics of objects in the % same cluster are similar and the characteristics of objects in different % clusters are dissimilar. % % This demo illustrates how to perform cluster analysis using the Statistics Toolbox(TM).  %   Copyright 2002-2011 The MathWorks, Inc. %   $Revision: 1.1.8.5 $  $Date: 2012/02/14 03:55:30 $  %% K-Means and Hierarchical Clustering % The Statistics Toolbox includes functions to perform two types of cluster % analysis, K-means clustering and hierarchical clustering. % % K-means clustering is a partitioning method that treats observations in % your data as objects having locations and distances from each other. It % partitions the objects into K mutually exclusive clusters, such that % objects within each cluster are as close to each other as possible, and % as far from objects in other clusters as possible. Each cluster is % characterized by its centroid, or center point. Of course, the distances % used in clustering often do not represent spatial distances. % % Hierarchical clustering is a way to investigate grouping in your data, % simultaneously over a variety of scales of distance, by creating a % cluster tree. The tree is not a single set of clusters, as in K-Means, % but rather a multi-level hierarchy, where clusters at one level are % joined as clusters at the next higher level. This allows you to decide % what scale or level of clustering is most appropriate in your % application.  %% % Some of the functions used in the example call MATLAB(R) built-in random % number generation functions.  To duplicate the exact results shown in % this demo, you should execute the command below, to set the random % number generator to a known state.  If you do not set the state, your % results may differ in trivial ways, for example, you may see clusters % numbered in a different order.  There is also a chance that a suboptimal % cluster solution may result (the demo includes a discussion of suboptimal % solutions, including ways to avoid them). rng(14,'twister');  %% Fisher's Iris Data % In the 1920's, botanists collected measurements on the sepal length, % sepal width, petal length, and petal width of 150 iris specimens, 50 from % each of three species. The measurements became known as Fisher's iris % data. % % Each observation in these data comes from a known species, and so there % is already an obvious way to group the data. For the moment, we will % ignore the species information and cluster the data using only the raw % measurements. When we're done, we can compare the resulting clusters to % the actual species, to see if the three types of iris possess distinct % characteristics.   %% Clustering Fisher's Iris Data Using K-Means Clustering % The function |kmeans| performs K-Means clustering, using an iterative % algorithm that assigns objects to clusters so that the sum of distances % from each object to its cluster centroid, over all clusters, is a % minimum. Used on Fisher's iris data, it will find the natural groupings % among iris specimens, based on their sepal and petal measurements. With % K-means clustering, you must specify the number of clusters that you want % to create. % % First, load the data and call |kmeans| with the desired number of clusters % set to 2, and using squared Euclidean distance.  To get an idea of how % well-separated the resulting clusters are, you can make a silhouette % plot. The silhouette plot displays a measure of how close each point in % one cluster is to points in the neighboring clusters. load fisheriris [cidx2,cmeans2] = kmeans(meas,2,'dist','sqeuclidean'); [silh2,h] = silhouette(meas,cidx2,'sqeuclidean');  %% % From the silhouette plot, you can see that most points in both clusters % have a large silhouette value, greater than 0.8, indicating that those % points are well-separated from neighboring clusters. However, each % cluster also contains a few points with low silhouette values, indicating % that they are nearby to points from other clusters. % % It turns out that the fourth measurement in these data, the petal width, % is highly correlated with the third measurement, the petal length, and so % a 3-D plot of the first three measurements gives a good representation of % the data, without resorting to four dimensions.  If you plot the data, % using different symbols for each cluster created by |kmeans|, you can % identify the points with small silhouette values, as those points that % are close to points from other clusters. ptsymb = {'bs','r^','md','go','c+'}; for i = 1:2     clust = find(cidx2==i);     plot3(meas(clust,1),meas(clust,2),meas(clust,3),ptsymb{i});     hold on end plot3(cmeans2(:,1),cmeans2(:,2),cmeans2(:,3),'ko'); plot3(cmeans2(:,1),cmeans2(:,2),cmeans2(:,3),'kx'); hold off xlabel('Sepal Length'); ylabel('Sepal Width'); zlabel('Petal Length'); view(-137,10); grid on  %% % The centroids of each cluster are plotted using circled X's. Three of the % points from the lower cluster, plotted with triangles, are very close to % points from the upper cluster, plotted with squares. But, in fact, % because the upper cluster is so spread out, those three points are closer % to the centroid of the lower cluster than to that of the upper cluster, % even though they are separated from the bulk of the points in their own % cluster by a gap. Because K-means clustering only considers distances, % and not densities, this kind of result can occur. % % You can increase the number of clusters to see if |kmeans| can find further % grouping structure in the data. This time, use the optional 'display' % parameter to print out information about each iteration in the clustering % algorithm. [cidx3,cmeans3] = kmeans(meas,3,'display','iter');  %% % At each iteration, the |kmeans| algorithm reassigns points among clusters % to decrease the sum of point-to-centroid distances, and then recomputes % cluster centroids for the new cluster assignments.  Notice that the total % sum of distances and the number of reassignments decrease at each % iteration until the algorithm reaches a minimum.  The algorithm used in % |kmeans| consists of two phases.  In the example here, the second phase of % the algorithm did not make any reassignments, indicating that the first % phase reached a minimum after only a few iterations. % % By default, |kmeans| begins the clustering process using a randomly % selected set of initial centroid locations. Just as in many other types % of numerical minimizations, the solution that |kmeans| reaches sometimes % depends on the starting points, and it is possible for it to reach a % local minimum, where reassigning any one point to a new cluster would % increase the total sum of point-to-centroid distances, but where a better % solution does exist. However, you can use the optional 'replicates' % parameter to overcome that problem.  When you specify more than one % replicate, |kmeans| repeats the clustering process starting from different % randomly selected centroids for each replicate. [cidx3,cmeans3,sumd3] = kmeans(meas,3,'replicates',5,'display','final');  %% % The output shows that, even for this relatively simple problem, % non-global minima do exist.  Each of these five replicates began from a % different set of initial centroids.  Depending on where it started from, % |kmeans| reached one of two different solutions. However, the final % solution that |kmeans| returns is the one with the lowest total sum of % distances, over all replicates.  The third output argument contains the % sum of distances within each cluster for that best solution. sum(sumd3)  %% % A silhouette plot for this three-cluster solution indicates that there is % one cluster that is well-separated, but that the other two clusters are % not very distinct. [silh3,h] = silhouette(meas,cidx3,'sqeuclidean');  %% % Again, you can plot the raw data to see how |kmeans| has assigned the % points to clusters. for i = 1:3     clust = find(cidx3==i);     plot3(meas(clust,1),meas(clust,2),meas(clust,3),ptsymb{i});     hold on end plot3(cmeans3(:,1),cmeans3(:,2),cmeans3(:,3),'ko'); plot3(cmeans3(:,1),cmeans3(:,2),cmeans3(:,3),'kx'); hold off xlabel('Sepal Length'); ylabel('Sepal Width'); zlabel('Petal Length'); view(-137,10); grid on  %% % You can see that |kmeans| has split the upper cluster from the two-cluster % solution, and that those two clusters are very close to each other. % Depending on what you intend to do with these data after clustering them, % this three-cluster solution may be more or less useful than the previous, % two-cluster, solution.  The first output argument from |silhouette| % contains the silhouette values for each point, which you can use to % compare the two solutions quantitatively.  The average silhouette value % was larger for the two-cluster solution, indicating that it is a better % answer purely from the point of view of creating distinct clusters. [mean(silh2) mean(silh3)]  %% % You can also cluster these data using a different distance. The cosine % distance might make sense for these data because it would ignore % absolute sizes of the measurements, and only consider their relative sizes. % Thus, two flowers that were different sizes, but which had similarly shaped % petals and sepals, might not be close with respect to squared Euclidean % distance, but would be close with respect to cosine distance. [cidxCos,cmeansCos] = kmeans(meas,3,'dist','cos');  %% % From the silhouette plot, these clusters appear to be only slightly % better separated than those found using squared Euclidean distance. [silhCos,h] = silhouette(meas,cidxCos,'cos'); [mean(silh2) mean(silh3) mean(silhCos)]  %% % Notice that the order of the clusters is different than in the previous % silhouette plot.  This is because |kmeans| chooses initial cluster % assignments at random. % % By plotting the raw data, you can see the differences in the cluster % shapes created using the two different distances. The two solutions are % similar, but the two upper clusters are elongated in the direction of % the origin when using cosine distance. for i = 1:3     clust = find(cidxCos==i);     plot3(meas(clust,1),meas(clust,2),meas(clust,3),ptsymb{i});     hold on end hold off xlabel('Sepal Length'); ylabel('Sepal Width'); zlabel('Petal Length'); view(-137,10); grid on  %% % This plot does not include the cluster centroids, because a centroid with % respect to the cosine distance corresponds to a half-line from the origin % in the space of the raw data. However, you can make a parallel coordinate % plot of the normalized data points to visualize the differences between % cluster centroids. lnsymb = {'b-','r-','m-'}; names = {'SL','SW','PL','PW'}; meas0 = meas ./ repmat(sqrt(sum(meas.^2,2)),1,4); ymin = min(min(meas0)); ymax = max(max(meas0)); for i = 1:3     subplot(1,3,i); plot(meas0(cidxCos==i,:)',lnsymb{i});     hold on; plot(cmeansCos(i,:)','k-','LineWidth',2); hold off;     title(sprintf('Cluster %d',i));     set(gca,'Xlim',[.9 4.1],'XTick',1:4,'XTickLabel',names,'YLim',[ymin ymax]) end  %% % It's clear from this plot that specimens from each of the three clusters % have distinctly different relative sizes of petals and sepals on average. % The first cluster has petals that are strictly smaller than their sepals. % The second two clusters' petals and sepals overlap in size, however, % those from the third cluster overlap more than the second.  You can also % see that the second and third clusters include some specimens which are % very similar to each other. % % Because we know the species of each observation in the data, you can % compare the clusters discovered by |kmeans| to the actual species, to see % if the three species have discernibly different physical % characteristics. In fact, as the following plot shows, the clusters % created using cosine distance differ from the species groups for only % five of the flowers. Those five points, plotted with stars, are all near % the boundary of the upper two clusters. subplot(1,1,1); for i = 1:3     clust = find(cidxCos==i);     plot3(meas(clust,1),meas(clust,2),meas(clust,3),ptsymb{i});     hold on end xlabel('Sepal Length'); ylabel('Sepal Width'); zlabel('Petal Length'); view(-137,10); grid on sidx = grp2idx(species); miss = find(cidxCos ~= sidx); plot3(meas(miss,1),meas(miss,2),meas(miss,3),'k*'); legend({'setosa','versicolor','virginica'}); hold off   %% Clustering Fisher's Iris Data Using Hierarchical Clustering % K-Means clustering produced a single partition of the iris data, but % you might also want to investigate different scales of grouping in % your data. Hierarchical clustering lets you do just that, by creating a % hierarchical tree of clusters. % % First, create a cluster tree using distances between observations in the % iris data.  Begin by using Euclidean distance. eucD = pdist(meas,'euclidean'); clustTreeEuc = linkage(eucD,'average');  %% % The cophenetic correlation is one way to verify that the cluster tree is % consistent with the original distances.  Large values indicate that the % tree fits the distances well, in the sense that pairwise linkages between % observations correlate with their actual pairwise distances.  This tree % seems to be a fairly good fit to the distances. cophenet(clustTreeEuc,eucD)  %% % To visualize the hierarchy of clusters, you can plot a dendrogram. [h,nodes] = dendrogram(clustTreeEuc,0); set(gca,'TickDir','out','TickLength',[.002 0],'XTickLabel',[]);  %% % The root node in this tree is much higher than the remaining nodes, % confirming what you saw from K-Means clustering: there are two large, % distinct groups of observations. Within each of those two groups, you can % see that lower levels of groups emerge as you consider smaller and % smaller scales in distance. There are many different levels of groups, of % different sizes, and at different degrees of distinctness.  %% % Based on the results from K-Means clustering, cosine might also be a good % choice of distance measure.  The resulting hierarchical tree is quite % different, suggesting a very different way to look at group structure in % the iris data. cosD = pdist(meas,'cosine'); clustTreeCos = linkage(cosD,'average'); cophenet(clustTreeCos,cosD) %% [h,nodes] = dendrogram(clustTreeCos,0); set(gca,'TickDir','out','TickLength',[.002 0],'XTickLabel',[]);  %% % The highest level of this tree separates iris specimens into two very % distinct groups.  The dendrogram shows that, with respect to cosine % distance, the within-group differences are much smaller relative to the % between-group differences than was the case for Euclidean distance.  This % is exactly what you would expect for these data, since the cosine % distance computes a zero pairwise distance for objects that are in the % same "direction" from the origin. % % With 150 observations, the plot is cluttered, but you can make a % simplified dendrogram that does not display the very lowest levels of the % tree. [h,nodes] = dendrogram(clustTreeCos,12);  %% % The three highest nodes in this tree separate out three equally-sized % groups, plus a single specimen (labeled as leaf node 5) that is not near % any others. [sum(ismember(nodes,[11 12 9 10])) sum(ismember(nodes,[6 7 8])) ...                   sum(ismember(nodes,[1 2 4 3])) sum(nodes==5)]  %% % For many purposes, the dendrogram might be a sufficient result.  However, % you can go one step further, and use the |cluster| function to cut the tree % and explicitly partition observations into specific clusters, as with % K-Means. Using the hierarchy from the cosine distance to create clusters, % specify a linkage height that will cut the tree below the three highest % nodes, and create four clusters, then plot the clustered raw data. hidx = cluster(clustTreeCos,'criterion','distance','cutoff',.006); for i = 1:5     clust = find(hidx==i);     plot3(meas(clust,1),meas(clust,2),meas(clust,3),ptsymb{i});     hold on end hold off xlabel('Sepal Length'); ylabel('Sepal Width'); zlabel('Petal Length'); view(-137,10); grid on %% % This plot shows that the results from hierarchical clustering with % cosine distance are qualitatively similar to results from K-Means, using % three clusters. However, creating a hierarchical cluster tree allows you % to visualize, all at once, what would require considerable % experimentation with different values for K in K-Means clustering. % % Hierarchical clustering also allows you to experiment with different % linkages. For example, clustering the iris data with single linkage, % which tends to link together objects over larger distances than average % distance does, gives a very different interpretation of the structure % in the data. clustTreeSng = linkage(eucD,'single'); [h,nodes] = dendrogram(clustTreeSng,0); set(gca,'TickDir','out','TickLength',[.002 0],'XTickLabel',[]);   displayEndOfDemoMessage(mfilename)  ##### SOURCE END ##### --></body></html>